%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\label{sec:vm-install}Setting Up the VM and Docker Universes}
%\section{\label{sec:vm-install}Setting Up the VM Universe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:vm-install}The VM Universe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{virtual machines}
\index{installation!for the vm universe}
\index{universe!set up for the vm universe}

\SubmitCmdNI{vm} universe jobs may
be executed on any execution site with VMware, Xen
(via \Prog{libvirt}), or KVM.
To do this, HTCondor must be informed of some details of the 
virtual machine installation, and the execution machines must
be configured correctly.

What follows is not a comprehensive list of the options that
help set up to use the \SubmitCmdNI{vm} universe; rather,
it is intended to serve as a starting point for those users interested in
getting \SubmitCmdNI{vm} universe jobs up and running quickly.
Details of configuration variables are in section~\ref{sec:Config-VMs}.

Begin by installing the virtualization package on all execute machines,
according to the vendor's instructions.
We have successfully used VMware, Xen, and KVM.
If considering running on a Windows system, 
a \Prog{Perl} distribution will also need to be installed;
we have successfully used \Prog{ActivePerl}. 

For VMware, \Prog{VMware Server 1} must be installed
and running on the execute machine.
HTCondor also
supports using \Prog{VMware Workstation} and \Prog{VMware Player}, version 5.
Earlier versions of these products may also work.  
HTCondor will attempt to automatically discern which 
VMware product is installed.
If using \Prog{Player}, also install the \Prog{VIX API},
which is freely available from VMware.

For Xen, there are three things that must exist on 
an execute machine to fully support \SubmitCmdNI{vm} universe jobs. 
\begin{enumerate}
\item
A Xen-enabled kernel must be running. 
This running Xen kernel acts as Dom0, in Xen terminology, 
under which all VMs are started, called DomUs Xen terminology. 

\item
The \Prog{libvirtd} daemon must be available,
and \Prog{Xend} services must be running. 

\item
The \Prog{pygrub} program must be available,
for execution of VMs whose disks contain the kernel they will run.
\end{enumerate}

For KVM, there are two things that must exist on
an execute machine to fully support \SubmitCmdNI{vm} universe jobs. 
\begin{enumerate}
\item
The machine must have the KVM kernel module installed and running.

\item
The \Prog{libvirtd} daemon must be installed and running.

\end{enumerate}

Configuration is required to enable the execution of
\SubmitCmdNI{vm} universe jobs.
The type of virtual machine that is installed on the
execute machine must be specified with the \Macro{VM\_TYPE} variable. 
For now, only one type can be utilized per machine.
For instance, the following tells HTCondor to use VMware:

\begin{verbatim}
VM_TYPE = vmware
\end{verbatim}

The location of the \Condor{vm-gahp} and
its log file must also be specified on the execute machine.
On a Windows installation, these options would look like this:

\begin{verbatim}
VM_GAHP_SERVER = $(SBIN)/condor_vm-gahp.exe
VM_GAHP_LOG = $(LOG)/VMGahpLog
\end{verbatim}

%You must also provide a version string for the Virtual Machine software
%you are using:

%\begin{verbatim}
%VM_VERSION = server1.0.4
%\end{verbatim}

%While required, this option does not alter the behavior of HTCondor.
%Instead, it is added to the ClassAd for the machine, so it 
%can be matched against.  This way, if future releases of VMware/Xen support
%new features that are desirable for your job, you can match on this string.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{VMware-Specific Configuration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To use VMware, identify the location of the \Prog{Perl} executable
on the execute machine.
In most cases, the default value should suffice:

\begin{verbatim}
VMWARE_PERL = perl
\end{verbatim}

This, of course, assumes the \Prog{Perl} executable is in the path
of the \Condor{master} daemon.
If this is not the case,
then a full path to the \Prog{Perl} executable will be required.

If using \Prog{VMware Player}, 
which does not support snapshots,
configure the \MacroNI{START} expression to reject
jobs which require snapshots.
These are jobs that do not have 
\SubmitCmd{vmware\_snapshot\_disk} set to \Expr{False}.
Here is an example modification to the \MacroNI{START} expression. 
\begin{verbatim}
START = ($(START)) && (!(TARGET.VMPARAM_VMware_SnapshotDisk =?= TRUE))
\end{verbatim}

The final required configuration is the location of the VMware control script
used by the \Condor{vm-gahp} on the execute machine
to talk to the virtual machine hypervisor.
It is located in HTCondor's \File{sbin} directory:

\begin{verbatim}
VMWARE_SCRIPT = $(SBIN)/condor_vm_vmware
\end{verbatim}

Note that an execute machine's \MacroNI{EXECUTE} variable should not
contain any symbolic links in its path,
if the machine is configured to run VMware \SubmitCmdNI{vm} universe jobs.
Strange behavior has been noted when HTCondor tries to run a 
\SubmitCmdNI{vm} universe VMware
job using a path to a VMX file that contains a symbolic link.
An example of an error message that may appear in such a job's event log:
\begin{verbatim}
Error from starter on master_vmuniverse_strtd@nostos.cs.wisc
.edu: register(/scratch/gquinn/condor/git/CONDOR_SRC/src/con
dor_tests/31426/31426vmuniverse/execute/dir_31534/vmN3hylp_c
ondor.vmx) = 1/Error: Command failed: A file was not found/(
ERROR) Can't create snapshot for vm(/scratch/gquinn/condor/g
it/CONDOR_SRC/src/condor_tests/31426/31426vmuniverse/execute
/dir_31534/vmN3hylp_condor.vmx)
\end{verbatim}
To work around this problem:
\begin{itemize}
\item If using file transfer
(the submit description file contains
\SubmitCmd{vmware\_should\_transfer\_files = true}),
then modify any
configuration variable \Macro{EXECUTE} values on all execute machines,
such that they do not contain symbolic link path components.
\item If using a shared file system, ensure that the
submit description file command
\SubmitCmd{vmware\_dir} does not use
symbolic link path name components.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Xen-Specific and KVM-Specific Configuration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Once the configuration options have been set, restart the \Condor{startd} 
daemon on that host.  For example:

\begin{verbatim}
> condor_restart -startd leovinus
\end{verbatim}

The \Condor{startd} daemon takes a few moments to exercise the VM
capabilities of the \Condor{vm-gahp}, query its properties, and then 
advertise the machine to the pool as VM-capable.
If the set up succeeded,
 then \Condor{status} will reveal that the host is now 
VM-capable by printing the VM type and the version number:

\begin{verbatim}
> condor_status -vm leovinus
\end{verbatim}

After a suitable amount of time, if this command does not give any output,
then the \Condor{vm-gahp} is having difficulty executing the VM software.
The exact cause of the problem depends on the details of the VM, the local 
installation, and a variety of other factors. We can offer only limited 
advice on these matters:

For Xen and KVM,
the \SubmitCmdNI{vm} universe is only available when \Login{root} starts HTCondor.
This is a restriction currently imposed because root privileges are 
required to create a virtual machine on top of a Xen-enabled kernel.
Specifically, root is needed 
to properly use the \Prog{libvirt} utility that controls 
creation and management of Xen and KVM guest virtual machines.
This restriction may be lifted in future versions,
depending on features provided by the underlying tool \Prog{libvirt}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{When a vm Universe Job Fails to Start}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

If a vm universe job should fail to launch, 
HTCondor will attempt to distinguish
between a problem with the user's job description,
and a problem with the virtual machine infrastructure of the 
matched machine.  
If the problem is with the job, 
the job will go on hold with a reason explaining the problem.  
If the problem is with the virtual machine infrastructure,
HTCondor will reschedule the job, and it will modify the machine ClassAd
to prevent any other vm universe job from matching.
vm universe configuration is not slot-specific, 
so this change is applied to all slots.

When the problem is with the virtual machine infrastructure,
these machine ClassAd attributes are changed:
\begin{itemize}
\item \Attr{HasVM} will be set to \Expr{False}
\item \Attr{VMOfflineReason} will be set to a somewhat explanatory string
\item \Attr{VMOfflineTime} will be set to the time of the failure 
\item \Attr{OfflineUniverses} will be adjusted to include \AdStr{VM} 
and \Expr{13}
\end{itemize}

Since \Condor{submit}
adds \Expr{HasVM == True} to a vm universe job's requirements,
no further 
vm universe jobs will match.

Once any problems with the infrastructure are fixed,
to change the machine ClassAd attributes such that the machine
will once again match to vm universe jobs,
an administrator has three options.  
All have the same effect
of setting the machine ClassAd attributes to the correct values
such that the machine will not reject matches for vm universe jobs.
\begin{enumerate}
\item  Restart the \Condor{startd} daemon.
\item  Submit a vm universe job that explicitly matches the machine.
When the job runs, the code detects the running job and causes 
the attributes related to the vm universe to be set indicating 
that vm universe jobs can match with this machine.
\item  Run the command line tool \Condor{update\_machine\_ad}
to set machine ClassAd attribute \Attr{HasVM} to \Expr{True},
and this will cause the other attributes related to the vm universe 
to be set indicating that vm universe jobs can match with this machine.
See the \Condor{update\_machine\_ad} manual page for
examples and details.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:docker-install}The Docker Universe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{docker universe!set up}
\index{installation!for the docker universe}
\index{universe!docker}
\index{universe!set up for the docker universe}
The execution of a docker universe job causes the
instantiation of a Docker container on an execute host.

The docker universe job is mapped to a vanilla universe job,
and the submit description file must specify 
the submit command \SubmitCmd{docker\_image} to identify the Docker image.
The job's \Attr{requirement} ClassAd attribute is automatically appended,
such that the job will only match with an execute machine 
that has Docker installed.

\index{ClassAd machine attribute!HasDocker}
The Docker service must be pre-installed on each execute machine
that can execute a docker universe job.
Upon start up of the \Condor{startd} daemon,
the capability of the execute machine to run docker universe jobs is probed, 
and the machine ClassAd attribute \Attr{HasDocker} is advertised
for a machine that is capable of running Docker universe jobs.

When a docker universe job is matched with a Docker-capable
execute machine,
HTCondor invokes the Docker CLI to instantiate the image-specific container.
The job's scratch directory tree is mounted as a Docker volume.
When the job completes, is put on hold, or is evicted, the container is removed.

In addition to installing the Docker service, 
the single configuration variable \Macro{DOCKER} must be set.
It defines the location of the Docker CLI and can also specify that
the \Condor{starter} daemon has been given a password-less sudo
permission to start the container as root.
Details of the \MacroNI{DOCKER} configuration variable are in
section~\ref{param:Docker}.

Docker may be installed as \Login{root} on a RedHat Linux machine these
ordered steps.
\begin{enumerate}
\item
Acquire and install the docker software:
\begin{verbatim}
  yum install docker-io
\end{verbatim}
Note that the \Prog{docker} package,
which manages the window manager's dock,
 may need to be uninstalled,
if it conflicts with this \Prog{docker-io} package.
\item
Set up the groups:
\begin{verbatim}
  useradd -G docker condor
\end{verbatim}
\item
Invoke the docker software:
\begin{verbatim}
  service docker start
\end{verbatim}
\item
Reconfigure the execute machine, such that it can set the machine ClassAd
attribute \Attr{HasDocker}:
\begin{verbatim}
  condor_reconfig
\end{verbatim}
\item
Check that the execute machine properly advertises that it is docker-capable
with:
\begin{verbatim}
  condor_status -l | grep -i docker
\end{verbatim}
The output of this command line for a correctly-installed and 
docker-capable execute host will be similar to
\begin{verbatim}
  HasDocker = true
  DockerVersion = "Docker Version 1.6.0, build xxxxx/1.6.0"
\end{verbatim}
\end{enumerate}

By default, HTCondor will keep the 20 most recently used Docker images
on the local machine.  This number may be controlled with the configuration
variable \Macro{DOCKER\_IMAGE\_CACHE\_SIZE}, 
to increase or decrease the number
of images, and the corresponding disk space, used by Docker.

An administrator of a machine can optionally make additional directories
on the host machine readable and writable by a running container.
To do this, the admin must first give an HTCondor name to each directory
with the DOCKER\_VOLUMES parameter.  Then, each volume must be configured
with the path on the host OS with the DOCKER\_VOLUME\_DIR\_XXX parameter.
Finally, the parameter DOCKER\_MOUNT\_VOLUMES tells HTCondor which of
these directories to always mount onto containers running on this machine.

For example,
\begin{verbatim}
DOCKER_VOLUMES = SOME_DIR, ANOTHER_DIR
DOCKER_VOLUME_DIR_SOME_DIR = /path1
DOCKER_VOLUME_DIR_ANOTHER_DIR = /path/to/no2
DOCKER_MOUNT_VOLUMES = SOME_DIR, ANOTHER_DIR
\end{verbatim}

The \Condor{startd} will advertise which docker volumes it has available 
for mounting with the machine attributes HasDockerVolumeSOME\_NAME = true
so that jobs can match to machines with volumes they need.

