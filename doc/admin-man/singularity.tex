%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Singularity Support}\label{sec:singularity-support}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{installation!Singularity}
\index{Singularity}

Note:  This documentation is very basic and needs improvement!

Here's an example configuration file:

\begin{verbatim}
  # Only set if singularity is not in $PATH.
  #SINGULARITY = /opt/singularity/bin/singularity

  # Forces _all_ jobs to run inside singularity.
  SINGULARITY_JOB = true

  # Forces all jobs to use the CernVM-based image.
  SINGULARITY_IMAGE_EXPR = "/cvmfs/cernvm-prod.cern.ch/cvm3"

  # Maps $_CONDOR_SCRATCH_DIR on the host to /srv inside the image.
  SINGULARITY_TARGET_DIR = /srv

  # Writable scratch directories inside the image.  Auto-deleted after the job exits.
  MOUNT_UNDER_SCRATCH = /tmp, /var/tmp
\end{verbatim}

This provides the user with no opportunity to select a specific image.
Here are some changes to the above example to allow the user to specify an
image path:

\begin{verbatim}
  SINGULARITY_JOB = !isUndefined(TARGET.SingularityImage)
  SINGULARITY_IMAGE_EXPR = TARGET.SingularityImage
\end{verbatim}

Then, users could add the following to their submit file
(note the quoting):

\begin{verbatim}
  +SingularityImage = "/cvmfs/cernvm-prod.cern.ch/cvm3"
\end{verbatim}

Finally, let's pick an image based on the OS -- not the filename:

\begin{verbatim}
  SINGULARITY_JOB = (TARGET.DESIRED_OS isnt MY.OpSysAndVer) && ((TARGET.DESIRED_OS is "CentOS6") || (TARGET.DESIRED_OS is "CentOS7"))
  SINGULARITY_IMAGE_EXPR = (TARGET.DESIRED_OS is "CentOS6") ? "/cvmfs/cernvm-prod.cern.ch/cvm3" : "/cvmfs/cms.cern.ch/rootfs/x86_64/centos7/latest"
\end{verbatim}

Then, the user adds to their submit file:

\begin{verbatim}
  +DESIRED_OS="CentOS6"
\end{verbatim}

That would cause the job to run on the native host for CentOS6 hosts
and inside a CentOS6 Singularity container on CentOS7 hosts.

By default, singularity will bind mount the scratch directory that contains transfered input files, 
working files, and other per-job information into the container.  The administrator can optionally
specific additional directories to be bind mounted into the container.  For example, if there is some common
shared input data located on a machine, or on a shared filesystem, this directory can be bind-mounted 
and be visible inside the container.  This is controlled by the configuration parameter
SINGULARITY\_BIND\_EXPR.  This is an expression, which is evaluated in the context of the machine and job ads,
and which should evaluated to a string which contains a space separated list of directories to mount.

So, to always bind mount a directory named /nfs into the image, and administrator could set
\begin{verbatim}
 SINGULARITY_BIND_EXPR = "/nfs"
\end{verbatim}

Or, if a trusted user is allowed to bind mount anything on the host, an expression could be
\begin{verbatim}
  SINGULARITY_BIND_EXPR = (Owner == "TrustedUser") ? SomeExpressionFromJob : ""
\end{verbatim}

Also, note that if the slot the job runs in is provisioned with GPUs, perhaps
in response to a RequestGPU line in the submit file, the Singularity flag
"--nv" will be passed to Singularity, which should make the appropriate
nvidia devices visible inside the container.

Finally, if an administrator wants to pass additional arguments to the singularity exec command that HTCondor
does not currently support, the parameter SINGULARITY\_EXTRA\_ARGUMENTS allows arbitraty additional parameters
to be passed to the singularity exec command.  For example, to pass the --nv argument, to allow the GPUs on the
host to be visible inside the container, an administrator could set

\begin{verbatim}
SINGULARITY_EXTRA_ARGUMENTS = --nv
\end{verbatim}

If Singularity is installed as non-setuid, the following flag must be set
for \Condor{ssh\_to\_job} to work.

\begin{verbatim}
SINGULARITY_IS_SETUID = false
\end{verbatim}
