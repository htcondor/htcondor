%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:Resource-Limits-Cgroup}Limiting Resource Usage Using Cgroups} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{resource limits with cgroups}
\index{limits!on resource usage with cgroup}
\index{cgroups!resource limits}

While the method described to limit a job's resource usage is portable,
and it should run on any Linux or BSD or Unix system, 
it suffers from one large flaw.  
The flaw is that resource limits imposed are per process, not per job.
An HTCondor job is often composed of many Unix processes.  
If the method of limiting resource usage with a user job wrapper
is used to impose a 2 Gigabyte memory limit, 
that limit applies to each process in the job individually.  
If a job created 100 processes, each using just under 2 Gigabytes, 
the job would continue without the resource limits kicking in.
Clearly, this is not what the machine owner intends.  
Moreover,
the memory limit only applies to the virtual memory size, 
not the physical memory size, or the resident set size.  
This can be a problem for jobs
that use the \Code{mmap} system call to map in a large chunk of virtual memory,
but only need a small amount of memory at one time.
Typically, the resource the
administrator would like to control is physical memory, 
because when that is in short supply, 
the machine starts paging, and can become unresponsive very quickly.

The \Condor{starter} can, using the Linux cgroup capability,
apply resource limits collectively to sets of jobs, 
and apply limits to the physical memory used by a set of processes.  
The main downside of this technique is that it is only 
available on relatively new Unix distributions such as RHEL 6 and Debian 6.
This technique also may require editing of system configuration files.

To enable cgroup-based limits, first enable cgroup-based tracking, as
described in section ~\ref{sec:CGroupTracking}.  
Once that is set,
the \Condor{starter} will create a cgroup for each job, and set two
attributes in that cgroup which control resource usage therein.  
These
two attributes are the cpu.shares attribute in the cpu controller, and one
of two attributes in the memory controller, either memory.limit\_in\_bytes, or 
memory.soft\_limit\_in\_bytes.  
The configuration variable \Macro{CGROUP\_MEMORY\_LIMIT\_POLICY} controls
whether the hard limit (the former) or the soft limit will be used.  
If \MacroNI{CGROUP\_MEMORY\_LIMIT\_POLICY} is set to the string \Expr{hard}, 
the hard limit will be used.
If set to \Expr{soft}, the soft limit will be used.  
Otherwise, no limit will be set if the value is \Expr{none}.
The default is \Expr{soft}.
If the hard limit is in force, then the total amount of physical memory
used by the sum of all processes in this job will not be allowed to exceed
the limit.  
If the processes try to allocate more memory, the allocation will
succeed, and virtual memory will be allocated, 
but no additional physical memory will be allocated.
The system will keep the amount of physical memory constant by swapping some
page from that job out of memory.  
However, if the soft limit is in place,
the job will be allowed to go over the limit if there is free memory 
available on the system.  
Only when there is contention between other processes for physical memory
will the system force physical memory into swap and push
the physical memory used towards the assigned limit.
The memory size used in both cases is the machine ClassAd attribute
\Attr{Memory}.
Note that \Attr{Memory} is 
a static amount when using static slots, but it is dynamic when partitionable 
slots are used.  That is, the limit is whatever the "Mem" column of
condor\_status reports for that slot.  If the job exceeds both the physical 
memory and swap space, the job will be killed by the Linux Out-of-Memory 
killer, and HTCondor will put the job on hold with an appropriate message.

If \MacroNI{CGROUP\_MEMORY\_LIMIT\_POLICY} is set, HTCondor will also also
use cgroups to limit the amount of swap space used by each job.  By default, 
the maximum amount of swap space used by each slot is the total amount of 
Virtual Memory in the slot, minus the amount of physical memory.  
Note that HTCondor measures virtual memory in kbytes, and physical 
memory in megabytes.  To prevent jobs with high
memory usage from thrashing and excessive paging, and force HTCondor to
put them on hold instead, you can set a lower limit on the amount of
swap space they are allowed to use.  With partitionable slots, this is
done in the per slot definition, and must be a percentage of the total
swap space on the system. For example,

\begin{verbatim}
NUM_SLOTS_TYPE_1 = 1
SLOT_TYPE_1_PARTITIONABLE = true
SLOT_TYPE_1 = cpus=100%,swap=10%
\end{verbatim}

Optionally, if the administrator sets the config file setting
\Macro{PROPORTIONAL\_SWAP\_ASSSIGNMENT} = true, the maximum amount 
of swap space per slot will be set to the same proportion of the total
swap as as the proportion of physical memory.  That is, if a slot
(static or dyanmic) has half of the physical memory of the machine,
it will be given half of the swap space.

In addition to memory, the \Condor{starter} can also control 
the total amount of CPU used by all processes within a job.
To do this, it writes a value
to the cpu.shares attribute of the cgroup cpu controller.  
The value it writes is copied from the \Attr{Cpus} attribute 
of the machine slot ClassAd multiplied by 100.
Again, like the \Attr{Memory} attribute, this value is fixed for static slots,
but dynamic under partitionable slots.  
This tells the operating system
to assign cpu usage proportionally to the number of cpus in the slot.  
Unlike memory, 
there is no concept of \Expr{soft} or \Expr{hard}, 
so this limit only applies when there is contention for the cpu.
That is, on an eight core machine, with
only a single, one-core slot running, and otherwise idle, the job running
in the one slot could consume all eight cpus concurrently with this limit
in play, if it is the only thing running.  
If, however, all eight slots where running jobs, 
with each configured for one cpu, the cpu usage would be assigned
equally to each job, regardless of the number of processes or threads in each job.

