%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:Resource-Limits-Cgroup}Limiting Resource Usage Using Cgroups} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{resource limits with cgroups}
\index{limits!on resource usage with cgroup}
\index{cgroups!resource limits}

While the method described to limit a job's resource usage is portable,
and it should run on any Linux or BSD or Unix system, 
it suffers from one large flaw.  
The flaw is that resource limits imposed are per process, not per job.
An HTCondor job is often composed of many Unix processes.  
If the method of limiting resource usage with a user job wrapper
is used to impose a 2 Gigabyte memory limit, 
that limit applies to each process in the job individually.  
If a job created 100 processes, each using just under 2 Gigabytes, 
the job would continue without the resource limits kicking in.
Clearly, this is not what the machine owner intends.  
Moreover,
the memory limit only applies to the virtual memory size, 
not the physical memory size, or the resident set size.  
This can be a problem for jobs
that use the \Code{mmap} system call to map in a large chunk of virtual memory,
but only need a small amount of memory at one time.
Typically, the resource the
administrator would like to control is physical memory, 
because when that is in short supply, 
the machine starts paging, and can become unresponsive very quickly.

The \Condor{starter} can, using the Linux cgroup capability,
apply resource limits collectively to sets of jobs, 
and apply limits to the physical memory used by a set of processes.  
The main downside of this technique is that it is only 
available on relatively new Unix distributions such as RHEL 6 and Debian 6.
This technique also may require editing of system configuration files.

To enable cgroup-based limits, first enable cgroup-based tracking, as
described in section ~\ref{sec:CGroupTracking}.  
Once that is set,
the \Condor{starter} will create a cgroup for each job, and set two
attributes in that cgroup which control resource usage therein.  
These
two attributes are the cpu.shares attribute in the cpu controller, and one
of two attributes in the memory controller, either memory.limit\_in\_bytes, or 
memory.soft\_limit\_in\_bytes.  
The configuration variable \Macro{MEMORY\_LIMIT} controls
whether the hard limit (the former) or the soft limit will be used.  
If \MacroNI{MEMORY\_LIMIT} is set to the string \Expr{hard}, 
the hard limit will be used.
If set to \Expr{soft}, the soft limit will be used.  
Otherwise, no limit will be set if the value is \Expr{none}.
The default is \Expr{none}.
If the hard limit is in force, then the total amount of physical memory
used by the sum of all processes in this job will not be allowed to exceed
the limit.  
If the processes try to allocate more memory, the allocation will
succeed, and virtual memory will be allocated, 
but no additional physical memory will be allocated.
The system will keep the amount of physical memory constant by swapping some
page from that job out of memory.  
However, if the soft limit is in place,
the job will be allowed to go over the limit if there is free memory 
available on the system.  
Only when there is contention between other processes for physical memory
will the system force physical memory into swap and push
the physical memory used towards the assigned limit.
The memory size used in both cases is the machine ClassAd attribute
\Attr{Memory}.
Note that \Attr{Memory} is 
a static amount when using static slots, but it is dynamic when partitionable 
slots are used.

In addition to memory, the \Condor{starter} can also control 
the total amount of CPU used by all processes within a job.
To do this, it writes a value
to the cpu.shares attribute of the cgroup cpu controller.  
The value it writes is copied from the \Attr{Cpus} attribute 
of the machine slot ClassAd.
Again, like the \Attr{Memory} attribute, this value is fixed for static slots,
but dynamic under partitionable slots.  
This tells the operating system
to assign cpu usage proportionally to the number of cpus in the slot.  
Unlike memory, 
there is no concept of \Expr{soft} or \Expr{hard}, 
so this limit only applies when there is contention for the cpu.
That is, on an eight core machine, with
only a single, one-core slot running, and otherwise idle, the job running
in the one slot could consume all eight cpus concurrently with this limit
in play, if it is the only thing running.  
If, however, all eight slots where running jobs, 
with each configured for one cpu, the cpu usage would be assigned
equally to each job, regardless of the number of processes in each job.

