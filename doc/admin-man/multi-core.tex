%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:Configuring-SMP}
Configuring the \Condor{startd} for Multi-Core Machines}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{SMP machines!configuration|(}
\index{multi-core machines!configuration|(}
This section describes how to configure the \Condor{startd} for multi-core
machines.
Machines with more than one CPU or core may
be configured to run more than one job at a time.
As always, owners of the resources have great flexibility in defining
the policy under which multiple jobs may run, suspend, vacate, etc.  

Multi-core machines are represented to the HTCondor system as
shared resources broken up into individual \Term{slots}.
Each slot can be matched and claimed by users for jobs.
Each slot is represented by an individual machine ClassAd.
In this way, each multi-core machine will appear to the HTCondor system as
a collection of separate slots.  
As an example, a multi-core machine named
\Expr{vulture.cs.wisc.edu} would appear to HTCondor as the
multiple machines, named \Expr{slot1@vulture.cs.wisc.edu},
\Expr{slot2@vulture.cs.wisc.edu},
\Expr{slot3@vulture.cs.wisc.edu}, and so on.

\index{dividing resources in  multi-core machines}
The way that the \Condor{startd} breaks up the
shared system resources into the different slots
is configurable.
All shared system resources, such as RAM, disk space, and swap space,
can be divided evenly among all the slots, with each
slot assigned one core.
Alternatively, 
\Term{slot types} are defined by configuration, 
so that resources can be unevenly divided.
Regardless of the scheme used, it is important
to remember that the goal is to create a representative slot ClassAd,
to be used for matchmaking with jobs.

HTCondor does not
directly enforce slot shared resource allocations, and jobs
are free to oversubscribe to shared resources.
Consider an example where two slots are each defined with 50\Percent\ of
available RAM.  The resultant ClassAd for each slot will advertise one
half the available RAM.  Users may submit jobs with RAM requirements
that match these slots.  However, jobs run on either slot are free to
consume more than 50\Percent\ of available RAM.  HTCondor will not
directly enforce a RAM utilization limit on either slot.  If a shared
resource enforcement capability is needed, 
it is possible to write a
policy that will evict a job that oversubscribes to shared
resources, as described in section \ref{sec:Config-SMP-Policy}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:SMP-Divide}
Dividing System Resources in Multi-core Machines}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Within a machine the shared system resources of
cores, RAM, swap space and disk space will be divided for
use by the slots. 
There are two main ways to go about dividing the resources of 
a multi-core machine:

\begin{description}

\item[Evenly divide all resources.]  
  By default, the \Condor{startd} will
  automatically	divide the machine into slots,
  placing one core in each slot, and evenly dividing
  all shared resources among the slots.
  The only specification may be how many slots are reported at a time.
  By default, all slots are reported to HTCondor.

  How many slots are reported at a time
  is accomplished by setting the configuration
  variable \Macro{NUM\_SLOTS} to the integer number of slots desired.
  If variable \MacroNI{NUM\_SLOTS} is not defined,
  it defaults to the number of cores within the machine.
  Variable \MacroNI{NUM\_SLOTS} may not be used to make HTCondor advertise more
  slots than there are cores on the machine.
  The number of cores is defined by \Macro{NUM\_CPUS}.

\item[Define slot types.]
  Instead of an even division of resources per slot,
  the machine may have definitions of \Term{slot types},
  where each type is provided with a fraction of shared system resources.
  Given the slot type definition, control how many of each
  type are reported at any given time with further configuration.

  Configuration variables define the slot types,
  as well as variables that list how much of each system resource goes
  to each slot type.  

  Configuration variable \Macro{SLOT\_TYPE\_<N>},
  where \verb@<N>@ is an integer 
  (for example, \MacroNI{SLOT\_TYPE\_1})
  defines the slot type.
  Note that there may be multiple slots of each type.  
  The number of slots created of a given type
  is configured with \MacroNI{NUM\_SLOTS\_TYPE\_<N>}.

The type can be defined by:
\begin{itemize}
  % \frac{1}{4} doesn't work
  %\item A simple fraction, such as \frac{1}{4}
  \item A simple fraction, such as 1/4
  \item A simple percentage, such as 25\Percent
  \item A comma-separated list of attributes, with a percentage,
	fraction, numerical value, or \Expr{auto} for each one.
  \item A comma-separated list that includes a blanket value that serves
        as a default for any resources not explicitly specified in the list.
\end{itemize}
A simple fraction or percentage describes the allocation
of the total system resources,
including the number of CPUS or cores.
A comma separated list allows a fine tuning of
the amounts for specific resources.

The number of CPUs
and the total amount of RAM in
the machine do not change over time.
For these attributes, specify either absolute values or
percentages of the total available amount (or \Expr{auto}).  
For example, in a machine with 128 Mbytes of RAM,
all the following definitions result in the same allocation amount.
\begin{verbatim}
SLOT_TYPE_1 = mem=64

SLOT_TYPE_1 = mem=1/2

SLOT_TYPE_1 = mem=50%

SLOT_TYPE_1 = mem=auto
\end{verbatim}

Amounts of disk space and swap space are dynamic, as they change over time.
For these, specify a percentage or fraction of the total
value that is allocated to each slot, instead of specifying absolute values.
As the total values of these resources change on the machine, each
slot will take its fraction of the total and report that as its
available amount.

The disk space allocated to each slot is taken from the disk partition
containing the slot's \MacroNI{EXECUTE} or \Macro{SLOT<N>\_EXECUTE} directory.
If every slot is in a different partition, 
then each one may be defined with up to
100\Percent\ for its disk share.  If some slots are in the same
partition, then their total is not allowed to exceed 100\Percent.

The four predefined attribute names are case insensitive when defining 
slot types.
The first letter of the attribute name distinguishes between
these attributes.
The four attributes, with several examples of acceptable names for each:
\begin{itemize}
  \item Cpus, C, c, cpu 
  \item ram, RAM, MEMORY, memory, Mem, R, r, M, m
  \item disk, Disk, D, d
  \item swap, SWAP, S, s, VirtualMemory, V, v
\end{itemize}

As an example, consider a
machine with 4 cores and 256 Mbytes of RAM.
Here are valid example slot type definitions. 
Types 1-3 are all equivalent to each other, as are types 4-6.  Note that
in a real configuration, all of these slot types would not
be used together,
because they add up to more than 100\Percent\ of the various system resources.
This configuration example also omits definitions of
\MacroNI{NUM\_SLOTS\_TYPE\_<N>}, to define the number of each slot type.

\begin{verbatim}
  SLOT_TYPE_1 = cpus=2, ram=128, swap=25%, disk=1/2

  SLOT_TYPE_2 = cpus=1/2, memory=128, virt=25%, disk=50%

  SLOT_TYPE_3 = c=1/2, m=50%, v=1/4, disk=1/2

  SLOT_TYPE_4 = c=25%, m=64, v=1/4, d=25%

  SLOT_TYPE_5 = 25%

  SLOT_TYPE_6 = 1/4
\end{verbatim}

The default value for each resource share is \Expr{auto}.  The share
may also be explicitly set to \Expr{auto}.  All slots with the value
\Expr{auto} for a given type of resource will evenly divide
whatever remains,
after subtracting out explicitly
allocated resources given in other slot definitions.  
For example, if one slot is
defined to use 10\Percent\ of the memory and the rest define it as
\Expr{auto} (or leave it undefined), then the rest of the slots will
evenly divide 90\Percent\ of the memory between themselves.

In both of the following examples, the disk share is set to \Expr{auto},
number of cores is 1, and everything else is 50\Percent:

\begin{verbatim}
SLOT_TYPE_1 = cpus=1, ram=1/2, swap=50%

SLOT_TYPE_1 = cpus=1, disk=auto, 50%
\end{verbatim}

Note that it is possible to set the configuration variables such
that they specify an impossible configuration.
If this occurs, the \Condor{startd} daemon fails after writing
a message to its log attempting to indicate the configuration
requirements that it could not implement.

In addition to the standard resources of CPUs, memory, disk, and swap,
the administrator may also define custom resources on 
a localized per-machine basis.
To implement this, 
a list of names of the local machine resources are defined using configuration 
variable \Macro{MACHINE\_RESOURCE\_NAMES}.
This example defines two resources,
a GPU and an actuator:
\begin{verbatim}
MACHINE_RESOURCE_NAMES = gpu, actuator
\end{verbatim}

The quantities of available resources are defined using configuration
variables of the form \Macro{MACHINE\_RESOURCE\_<name>},
where \Expr{<name>} is as defined by variable 
\MacroNI{MACHINE\_RESOURCE\_NAMES}, as shown in this example:
\begin{verbatim}
MACHINE_RESOURCE_gpu = 16
MACHINE_RESOURCE_actuator = 8
\end{verbatim}

Local machine resource names defined in this way may now be used in conjunction 
with \Macro{SLOT\_TYPE\_<N>}, using all the same syntax described
earlier in this section.
The following example demonstrates
the definition of static and partitionable slot types with local machine 
resources:
\begin{verbatim}
# declare one partitionable slot with half of the GPUs, 6 actuators, and
# 50% of all other resources:
SLOT_TYPE_1 = gpu=50%,actuator=6,50%
SLOT_TYPE_1_PARTITIONABLE = TRUE
NUM_SLOTS_TYPE_1 = 1

# declare two static slots, each with 25% of the GPUs, 1 actuator, and
# 25% of all other resources: 
SLOT_TYPE_2 = gpu=25%,actuator=1,25%
SLOT_TYPE_2_PARTITIONABLE = FALSE
NUM_SLOTS_TYPE_2 = 2
\end{verbatim}

A job may request these local machine resources using the 
syntax \SubmitCmd{request\_<name>}, 
as described in section~\ref{sec:SMP-dynamicprovisioning}.  
This example shows a portion of a submit description file 
that requests GPUs and an actuator:
\begin{verbatim}
universe = vanilla

# request two GPUs and one actuator:
request_gpu = 2
request_actuator = 1

queue
\end{verbatim}

The slot ClassAd will represent each local machine resource
with the following attributes:
\begin{description}
\item{\Attr{Total<name>}: the total quantity of the resource 
  identified by \Expr{<name>}}
\item{\Attr{Detected<name>}: the quantity detected of the resource
  identified by \Expr{<name>}; this attribute is
  currently equivalent to \Attr{Total<name>}}
\item{\Attr{TotalSlot<name>}: the quantity of the resource
  identified by \Expr{<name>} allocated to this slot}
\item{\Attr{<name>}: the amount of the resource
  identified by \Expr{<name>} available to be used on this slot}
\end{description}

From the example given, the \Expr{gpu} resource would be represented by
the ClassAd attributes
\Attr{TotalGpu}, \Attr{DetectedGpu}, \Attr{TotalSlotGpu}, and \Attr{Gpu}.
In the job ClassAd, 
the amount of the requested machine resource appears 
in a job ClassAd attribute named \Attr{Request<name>}.
For this example,
the two attributes will be \Attr{RequestGpu} and \Attr{RequestActuator}.

The number of each type being
reported can be changed at run time, by issuing a reconfiguration
command to
the \Condor{startd} daemon (sending a SIGHUP or using \Condor{reconfig}).
However, the definitions for the types themselves cannot be changed
with reconfiguration.
To change any slot type definitions, use \Condor{restart}
\begin{verbatim}
condor_restart -startd
\end{verbatim}
for that change to take effect.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:Config-SMP-Policy}
Configuration Specific to Multi-core Machines}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{configuration!SMP machines}
\index{configuration!multi-core machines}
Each slot within a multi-core machine is treated as an
independent machine,
each with its own view of its state as represented by the
machine ClassAd attribute \Attr{State}.
The policy expressions for the multi-core machine
as a whole are propagated from the \Condor{startd}
to the slot's machine ClassAd.
This policy may consider a slot state(s) in its expressions.
This makes some policies easy to set, but it makes
other policies difficult or impossible to set.

An easy policy to set
configures how many of the slots
notice console or tty activity on the multi-core machine as a whole.
Slots that are not configured to notice any activity will report
\Attr{ConsoleIdle} and \Attr{KeyboardIdle} times from when the
\Condor{startd} daemon was started,
plus a configurable number of seconds.
A multi-core machine with the default policy
settings can add the keyboard and console to be noticed by only one slot.
Assuming a reasonable load average,
only the one slot will suspend or vacate its job
when the owner starts typing at their machine again.
The rest of the slots could be matched with jobs and continue running them,
even while the user was interactively using the
machine. 
If the default policy is used,
all slots notice
tty and console activity
and
currently running jobs would suspend.

This example policy is
controlled with the following configuration variables.
\begin{itemize}
\item \Macro{SLOTS\_CONNECTED\_TO\_CONSOLE}, with definition at
section~\ref{param:SlotsConnectedToConsole}
\item \Macro{SLOTS\_CONNECTED\_TO\_KEYBOARD}, with definition at
section~\ref{param:SlotsConnectedToKeyboard}
\item \Macro{DISCONNECTED\_KEYBOARD\_IDLE\_BOOST}, with definition at
section~\ref{param:DisconnectedKeyboardIdleBoost}
\end{itemize}

Each slot has its own machine ClassAd.
Yet, the policy expressions for the multi-core machine are
propagated and inherited from configuration of the \Condor{startd}.
Therefore, the policy expressions for each slot are the same.
This makes the implementation of certain types of policies impossible,
because while evaluating the state of one slot within the multi-core machine,
the state of other slots are not available.
Decisions for one slot cannot be based on what other slots are doing.

Specifically, the evaluation of a slot policy expression works in
the following way.
\begin{enumerate}
\item 
The configuration file specifies policy expressions that are shared by
all of the slots on the machine.
\item 
Each slot reads the configuration file and sets up its own machine ClassAd.
\item 
Each slot is now separate from the others.  It has a
different ClassAd attribute \Attr{State},
a different machine ClassAd, 
and if there is a job running, a separate job ClassAd.
Each slot periodically
evaluates the policy expressions, changing its own state as necessary.
This occurs independently of the other slots on the machine.
So, if the \Condor{startd} daemon is evaluating a policy expression
on a specific slot,
and the policy expression refers to \Attr{ProcID}, \Attr{Owner},
or any attribute from a job ClassAd,
it \emph{always} refers to the ClassAd of the
job running on the specific slot.
\end{enumerate}

To set a different policy for the slots within a machine,
incorporate the slot-specific machine ClassAd attribute \Attr{SlotID}.
A \MacroNI{SUSPEND} policy that is different for each of the two slots
will be of the form
\begin{verbatim}
SUSPEND = ( (SlotID == 1) && (PolicyForSlot1) ) || \
          ( (SlotID == 2) && (PolicyForSlot2) )
\end{verbatim}
where \verb@(PolicyForSlot1)@ and \verb@(PolicyForSlot2)@ are the
desired expressions for each slot.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:Multi-core-Load}
Load Average for Multi-core Machines}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{ClassAd machine attribute!CondorLoadAvg}
\index{ClassAd machine attribute!LoadAvg}
\index{ClassAd machine attribute!TotalCondorLoadAvg}
\index{ClassAd machine attribute!TotalLoadAvg}
Most operating systems define the load average for a multi-core machine as
the total load on all cores.
For example, a 4-core machine with 3 CPU-bound processes
running at the same time will have a load of 3.0.
In HTCondor, we maintain this view of the total load average and publish
it in all resource ClassAds as \Attr{TotalLoadAvg}.

HTCondor also provides a per-core load average for multi-core machines.
This nicely represents the model that each node on a multi-core machine
is a slot,
separate from the other nodes.
All of the default, single-core policy expressions can be used directly
on multi-core machines, without modification, since the \Attr{LoadAvg} and
\Attr{CondorLoadAvg} attributes are the per-slot versions,
not the total, multi-core wide versions.

The per-core load average on multi-core machines is an HTCondor invention. 
No system call exists to ask the operating system for this value.
HTCondor already computes the load average generated by HTCondor on each
slot.
It does this by close monitoring of all processes spawned by any of the
HTCondor daemons, even ones that are orphaned and then inherited by
\Prog{init}. 
This HTCondor load average per slot is reported as
the attribute
\Attr{CondorLoadAvg} in all resource ClassAds, and the total HTCondor
load average for the entire machine is reported as
\Attr{TotalCondorLoadAvg}. 
The total, system-wide load average for the entire
machine  is reported as \Attr{TotalLoadAvg}.
Basically, HTCondor walks through all the slots and assigns out
portions of the total load average to each one. 
First, HTCondor assigns the known HTCondor load average to each node that
is generating load.  
If there is any load average left in the total system load, 
it is considered an owner load.
Any slots HTCondor believes are in the Owner state,
such as ones that have keyboard activity,
are the first to get assigned this owner load.
HTCondor hands out owner load in increments of at most 1.0, so generally
speaking, no slot has a load average above 1.0.
If HTCondor runs out of total load average before it runs out of slots,
all the remaining machines believe that they have no load average at all.
If, instead, HTCondor runs out of slots and it still has owner
load remaining, HTCondor starts assigning that load to HTCondor nodes as
well,
giving individual nodes with a load average higher than 1.0.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:SMP-logging}
Debug Logging in the Multi-Core \Condor{startd} Daemon}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section describes how the \Condor{startd} daemon
handles its debugging messages for multi-core machines.
In general, a given log message will either be something that is
machine-wide, 
such as reporting the total system load average,
or it will be specific to a given slot.
Any log entries specific to a slot have an extra word 
printed out in the entry with the slot number.  
So, for example, here's the output about system resources that are
being gathered (with \Dflag{FULLDEBUG} and \Dflag{LOAD} turned on) on
a 2-core machine with no HTCondor activity, and the keyboard connected to
both slots:
\begin{verbatim}
11/25 18:15 Swap space: 131064
11/25 18:15 number of Kbytes available for (/home/condor/execute): 1345063
11/25 18:15 Looking up RESERVED_DISK parameter
11/25 18:15 Reserving 5120 Kbytes for file system
11/25 18:15 Disk space: 1339943
11/25 18:15 Load avg: 0.340000 0.800000 1.170000
11/25 18:15 Idle Time: user= 0 , console= 4 seconds
11/25 18:15 SystemLoad: 0.340   TotalCondorLoad: 0.000  TotalOwnerLoad: 0.340
11/25 18:15 slot1: Idle time: Keyboard: 0        Console: 4
11/25 18:15 slot1: SystemLoad: 0.340  CondorLoad: 0.000  OwnerLoad: 0.340
11/25 18:15 slot2: Idle time: Keyboard: 0        Console: 4
11/25 18:15 slot2: SystemLoad: 0.000  CondorLoad: 0.000  OwnerLoad: 0.000
11/25 18:15 slot1: State: Owner           Activity: Idle
11/25 18:15 slot2: State: Owner           Activity: Idle
\end{verbatim}

If, on the other hand, this machine only had one slot
connected to the keyboard and console, and the other slot was running a
job, it might look something like this:
\begin{verbatim}
11/25 18:19 Load avg: 1.250000 0.910000 1.090000
11/25 18:19 Idle Time: user= 0 , console= 0 seconds
11/25 18:19 SystemLoad: 1.250   TotalCondorLoad: 0.996  TotalOwnerLoad: 0.254
11/25 18:19 slot1: Idle time: Keyboard: 0        Console: 0
11/25 18:19 slot1: SystemLoad: 0.254  CondorLoad: 0.000  OwnerLoad: 0.254
11/25 18:19 slot2: Idle time: Keyboard: 1496     Console: 1496
11/25 18:19 slot2: SystemLoad: 0.996  CondorLoad: 0.996  OwnerLoad: 0.000
11/25 18:19 slot1: State: Owner           Activity: Idle
11/25 18:19 slot2: State: Claimed         Activity: Busy
\end{verbatim}

Shared system resources are printed without the header,
such as total swap space,
and slot-specific messages,
such as the load average or state of each slot,
get the slot number appended.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:GPU-resources}
Configuring GPUs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{GPUs!configuration}
\index{configuration!to use GPUs}
HTCondor supports incorporating GPU resources and making them available 
for jobs.
First, GPUs must be detected as available resources.
Then, machine ClassAd attributes advertise this availability.
Both detection and advertisement are accomplished by having
this configuration for each execute machine that has GPUs:
\begin{verbatim}
  use feature : GPUs
\end{verbatim}

Use of this metaknob invokes the \Condor{gpu\_discovery} tool
to create a custom resource, with a custom
resource name of \Expr{GPUs},
and it generates the ClassAd attributes needed to advertise the GPUs.
\Condor{gpu\_discovery} is invoked in a mode that discovers 
and advertises both CUDA and OpenCL GPUs.

This metaknob refers to macro \MacroNI{GPU\_DISCOVERY\_EXTRA},
which may be used to define additional command line arguments 
for the \Condor{gpu\_discovery} tool.
For example, setting 
\begin{verbatim}
  use feature : GPUs
  GPU_DISCOVERY_EXTRA = -extra
\end{verbatim}
causes the \Condor{gpu\_discovery} tool to output more attributes that
describe the detected GPUs on the machine.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:SMP-exprs}
Configuring STARTD\_ATTRS on a per-slot basis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \Macro{STARTD\_ATTRS} (and legacy \MacroNI{STARTD\_EXPRS}) settings
can be configured on a per-slot basis.
The \Condor{startd} daemon builds the list of items to
advertise by combining the lists in this order:
\begin{enumerate}
\item{\MacroNI{STARTD\_ATTRS}}
\item{\MacroNI{STARTD\_EXPRS}}
\item{\MacroNI{SLOT<N>\_STARTD\_ATTRS}}
\item{\MacroNI{SLOT<N>\_STARTD\_EXPRS}}
\end{enumerate}

For example, consider the following configuration:
\begin{verbatim}
STARTD_ATTRS = favorite_color, favorite_season
SLOT1_STARTD_ATTRS = favorite_movie
SLOT2_STARTD_ATTRS = favorite_song
\end{verbatim}

This will result in the \Condor{startd} ClassAd for
slot1 defining values for
\Attr{favorite\_color}, \Attr{favorite\_season},
and \Attr{favorite\_movie}.
Slot2 will have values for
\Attr{favorite\_color}, \Attr{favorite\_season}, and \Attr{favorite\_song}.

Attributes themselves in the \Expr{STARTD\_ATTRS} list
can also be defined on a per-slot basis.
Here is another example:

\begin{verbatim}
favorite_color = "blue"
favorite_season = "spring"
STARTD_ATTRS = favorite_color, favorite_season
SLOT2_favorite_color = "green"
SLOT3_favorite_season = "summer"
\end{verbatim}

For this example, the \Condor{startd} ClassAds are
\begin{description}
\item{slot1}:
\begin{verbatim}
favorite_color = "blue"
favorite_season = "spring"
\end{verbatim}
\item{slot2}:
\begin{verbatim}
favorite_color = "green"
favorite_season = "spring"
\end{verbatim}
\item{slot3}:
\begin{verbatim}
favorite_color = "blue"
favorite_season = "summer"
\end{verbatim}
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:SMP-dynamicprovisioning}
Dynamic Provisioning: Partitionable and Dynamic Slots}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{dynamic \Condor{startd} provisioning}
\index{slots!dynamic \Condor{startd} provisioning}
\index{slots!subdividing slots}
\index{dynamic slots}
\index{partitionable slots}

\Term{Dynamic provisioning},
also referred to as partitionable or dynamic slots,
allows HTCondor to use the resources of a slot in a dynamic way;
these slots may be partitioned. 
This means that more than one job can occupy a single slot at any one time. 
Slots have a fixed set of resources which
include the cores, memory and disk space. 
By partitioning the slot, 
the use of these resources becomes more flexible.

Here is an example that demonstrates how resources are divided
as more than one job
is or can be matched to a single slot.
In this example, Slot1 is identified as a partitionable slot
and has the following resources:
\begin{description}
  \item{cpu = 10}
  \item{memory = 10240}
  \item{disk = BIG}
\end{description}
Assume that JobA is allocated to this slot.
JobA includes the following requirements:
\begin{description}
  \item{cpu = 3}
  \item{memory = 1024}
  \item{disk = 10240} 
\end{description}
The portion of the slot that is carved out is now
known as a dynamic slot.
This dynamic slot has its own machine ClassAd, 
and its \Attr{Name} attribute
distinguishes itself as a dynamic slot with incorporating the substring
\Expr{Slot1\_1}.

After allocation, the partitionable Slot1 advertises that it has
the following resources still available:
\begin{description}
  \item{cpu = 7}
  \item{memory = 9216}
  \item{disk = BIG-10240}
\end{description}
As each new job is allocated to Slot1,
it breaks into \Expr{Slot1\_1}, \Expr{Slot1\_2}, \Expr{Slot1\_3} etc.,
until the entire set of
Slot1's available resources have been consumed by jobs.

To enable dynamic provisioning, 
define a slot type with machine resources.
Then,
identify that slot type as partitionable by setting
configuration variable 
\Macro{SLOT\_TYPE\_<N>\_PARTITIONABLE} 
to \Expr{True}.
The value of \Expr{<N>} within the configuration variable name
is the same value as in slot type definition configuration variable
\MacroNI{SLOT\_TYPE\_<N>}.
For the most common cases the machine should
be configured for one slot, managing all the resources on the machine.
To do so, set the following configuration variables:

\begin{verbatim}
NUM_SLOTS=1
NUM_SLOTS_TYPE_1=1
SLOT_TYPE_1=100%
SLOT_TYPE_1_PARTITIONABLE=true
\end{verbatim}

In a pool using dynamic provisioning, 
jobs can have extra, and desired, resources specified in the submit
description file:
\begin{description}
  \item{request\_cpus}
  \item{request\_memory}
  \item{request\_disk (in kilobytes)}
\end{description}

This example shows a portion of the job submit description file
for use when submitting a job to a pool with dynamic provisioning.
\begin{verbatim}
universe = vanilla

request_cpus = 3
request_memory = 1024
request_disk = 10240

queue 
\end{verbatim}

Each partitionable slot will have
the ClassAd attribute
\begin{verbatim}
  PartitionableSlot = True
\end{verbatim}
Each dynamic slot will have the ClassAd attribute 
\begin{verbatim}
  DynamicSlot = True
\end{verbatim}
These attributes may be used in a \MacroNI{START} expression for 
the purposes of creating detailed policies.

A partitionable slot will always appear as though it is not running a job.
If matched jobs consume all its resources,
the partitionable slot  will eventually show as having no available resources; 
this will prevent further matching of new jobs.
The dynamic slots will show as running jobs.
The dynamic slots can be preempted in the same way as all other slots.

Dynamic provisioning provides powerful configuration
possibilities, and so should be used with care. 
Specifically, while preemption occurs for each individual dynamic slot,
it cannot occur directly for the partitionable slot, 
or for groups of dynamic slots. 
For example, for a large number of jobs requiring 1GB of memory,
a pool might be split up into 1GB dynamic slots. 
In this instance a job requiring 2GB of memory will be starved
and unable to run.  A partial solution to this problem is provided
by defragmentation accomplished by the \Condor{defrag} daemon,
as discussed in section~\ref{sec:SMP-defrag}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:SMP-resource-defaults}
Defaults for Partitionable Slot Sizes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

If a job does not specify the required number of CPUs, amount of memory,
or disk space, there are ways for the administrator to set default
values for all of these parameters.

First, if any of these attributes are not set in the submit description file,
there are three variables in the configuration file that \condor{submit}
will use to fill in default values.  These are

\begin{description}
  \item{\Macro{JOB\_DEFAULT\_REQUESTMEMORY}}
  \item{\Macro{JOB\_DEFAULT\_REQUESTDISK}}
  \item{\Macro{JOB\_DEFAULT\_REQUESTCPUS}}
\end{description}

The value of these variables can be ClassAd expressions.  
The default values for these variables, should they not be set are

\begin{description}
  \item{\MacroNI{JOB\_DEFAULT\_REQUESTMEMORY} = \Expr{ifThenElse(MemoryUsage =!= UNDEFINED, MemoryUsage, 1)}}
  \item{\MacroNI{JOB\_DEFAULT\_REQUESTCPUS} = \Expr{1}}
  \item{\MacroNI{JOB\_DEFAULT\_REQUESTDISK} = \Expr{DiskUsage}}
\end{description}

Note that these default values are chosen such that 
jobs matched to partitionable slots function similar to static slots.

Once the job has been matched, 
and has made it to the execute machine, 
the \Condor{startd} has the ability to modify these 
resource requests before using them to size the
actual dynamic slots carved out of the partitionable slot.  
Clearly, for the job to work,
the \Condor{startd} daemon must create slots with at least 
as many resources as the job needs.  
However,
it may be valuable to create dynamic slots somewhat bigger 
than the job's request, 
as subsequent jobs may be more likely to reuse the newly created slot 
when the initial job is done using it.

The \Condor{startd} configuration variables which control this 
and their defaults are

\begin{description}
  \item{\Macro{MODIFY\_REQUEST\_EXPR\_REQUESTCPUS} = \Expr{quantize(RequestCpus, \{1\})}}
  \item{\Macro{MODIFY\_REQUEST\_EXPR\_REQUESTMEMORY} = \Expr{quantize(RequestMemory, \{128\}) }}
  \item{\Macro{MODIFY\_REQUEST\_EXPR\_REQUESTDISK} = \Expr{quantize(RequestDisk, \{1024\}) }}
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:consumption-policy}
condor\_negotiator-Side Resource Consumption Policies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{consumption policy}
\index{partitionable slots!negotiator-side resource consumption policy}
For partitionable slots, 
the specification of a consumption policy permits matchmaking
at the negotiator.
A dynamic slot carved from the partitionable slot acquires the
required quantities of resources, leaving the partitionable
slot with the remainder. 
This differs from scheduler matchmaking in that multiple
jobs can match with the partitionable slot during a single
negotiation cycle.

All specification of the resources available is done by configuration 
of the partitionable slot.  The machine is identified
as having a resource consumption policy enabled with
\begin{verbatim}
  CONSUMPTION_POLICY = True
\end{verbatim}
A defined slot type that is partitionable may override the
machine value with
\begin{verbatim}
  SLOT_TYPE_<N>_CONSUMPTION_POLICY = True
\end{verbatim}

A job seeking a match may always request a specific number of cores,
amount of memory, and amount of disk space.
Availability of these three resources on a machine and
within the partitionable slot
is always defined and have these default values:
\begin{verbatim}
  CONSUMPTION_CPUS = quantize(target.RequestCpus,{1})
  CONSUMPTION_MEMORY = quantize(target.RequestMemory,{128})
  CONSUMPTION_DISK = quantize(target.RequestDisk,{1024})
\end{verbatim}

Here is an example-driven definition of a consumption policy.
Assume a single partitionable slot type on a multi-core machine with
8 cores,
and that the resource this policy cares about allocating are the cores.
Configuration for the machine includes the definition of the slot type 
and that it is partitionable.
\begin{verbatim}
  SLOT_TYPE_1 = cpus=8
  SLOT_TYPE_1_PARTITIONABLE = True
  NUM_SLOTS_TYPE_1 = 1
\end{verbatim}

Enable use of the \Condor{negotiator}-side resource consumption
policy, allocating the job-requested number of cores to the
dynamic slot, and use \Macro{SLOT\_WEIGHT} to assess the user usage that 
will affect user priority by the number of cores allocated.
\begin{verbatim}
  SLOT_TYPE_1_CONSUMPTION_POLICY = True
  SLOT_TYPE_1_CONSUMPTION_CPUS = TARGET.RequestCpus
  SLOT_WEIGHT = Cpus
\end{verbatim}

If custom resources are available within the partitionable slot,
they may be used in a consumption policy, by specifying the 
resource.
Using a machine with 4 GPUs as an example custom resource,
define the resource and include it in the definition of the partitionable
slot:
\begin{verbatim}
  MACHINE_RESOURCE_NAMES = gpus
  MACHINE_RESOURCE_gpus = 4
  SLOT_TYPE_2 = cpus=8, gpus=4
  SLOT_TYPE_2_PARTITIONABLE = True
  NUM_SLOTS_TYPE_2 = 1
\end{verbatim}

Add the consumption policy to incorporate availability of the GPUs:
\begin{verbatim}
  SLOT_TYPE_2_CONSUMPTION_POLICY = True
  SLOT_TYPE_2_CONSUMPTION_gpus = TARGET.RequestGpu
  SLOT_WEIGHT = Cpus
\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:SMP-defrag}
Defragmenting Dynamic Slots}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{HTCondor daemon!condor\_defrag@\Condor{defrag}}
\index{daemon!condor\_defrag@\Condor{defrag}}
\index{condor\_defrag daemon}

When partitionable slots are used, some attention must be given to the
problem of the starvation of large jobs due to the fragmentation of resources.
The problem is that over time the machine resources may become
partitioned into slots suitable for running small jobs.
If a sufficient number of these slots do not happen to become idle at the
same time on a machine, then a large job will not be able to claim that
machine, even if the large job has a better priority than the small jobs.

One way of addressing the partitionable slot fragmentation problem is
to periodically drain all jobs from fragmented machines so that they
become defragmented.  
The \Condor{defrag} daemon implements a configurable policy for doing that.
Its implementation is targeted at machines configured to run whole-machine
jobs and at machines that only have partitionable slots.
The draining of a machine 
configured to have both partitionable slots and static slots 
would have a negative impact on single slot jobs running in static slots.

To use this daemon,
\MacroNI{DEFRAG} must be added to \MacroNI{DAEMON\_LIST},
and the defragmentation policy must be configured.
Typically, only one instance of the \Condor{defrag} daemon would be
run per pool.  
It is a lightweight daemon that should not require a lot of system resources.

Here is an example configuration that puts the \Condor{defrag} daemon to work:

\begin{verbatim}
DAEMON_LIST = $(DAEMON_LIST) DEFRAG
DEFRAG_INTERVAL = 3600
DEFRAG_DRAINING_MACHINES_PER_HOUR = 1.0
DEFRAG_MAX_WHOLE_MACHINES = 20
DEFRAG_MAX_CONCURRENT_DRAINING = 10
\end{verbatim}

This example policy tells \Condor{defrag} to initiate draining 
jobs from 1 machine per hour,
but to avoid initiating new draining if there are 
20 completely defragmented machines or 10 machines in a draining state.
A full description of each configuration variable
used by the \Condor{defrag} daemon may be found in 
section~\ref{sec:Config-defrag}.

By default, when a machine is drained, existing jobs are gracefully evicted.
This means that each job will be allowed to use the remaining
time promised to it by \MacroNI{MaxJobRetirementTime}.  
If the job has not finished when the retirement time runs out, 
the job will be killed with a soft kill signal, 
so that it has an opportunity to save a checkpoint
(if the job supports this).
No new jobs will be allowed to start while the machine is draining.
To reduce unused time on the
machine caused by some jobs having longer retirement time than others,
the eviction of jobs with shorter retirement time is delayed until the
job with the longest retirement time needs to be evicted.

There is a trade off between reduced starvation and throughput.
Frequent draining of machines reduces the chance of starvation of
large jobs.  However, frequent draining reduces total throughput.
Some of the machine's resources may go unused during draining,
if some jobs finish before others.  
If jobs that cannot produce checkpoints are killed
because they run past the end of their retirement time during draining,
this also adds to the cost of draining.

To help gauge the costs of draining, the \Condor{startd} advertises
the accumulated time that was unused due to draining and the time
spent by jobs that were killed due to draining.  
These are advertised
respectively in the attributes \AdAttr{TotalMachineDrainingUnclaimedTime} and
\AdAttr{TotalMachineDrainingBadput}.  
The \Condor{defrag} daemon
averages these values across the pool and advertises the result in its
daemon ClassAd in the attributes \AdAttr{AvgDrainingBadput} and
\AdAttr{AvgDrainingUnclaimed}.  
Details of all attributes published by
the \Condor{defrag} daemon are described in
section~\ref{sec:Defrag-ClassAd-Attributes}.

The following command may be used to view the \Condor{defrag} daemon
ClassAd:

\begin{verbatim}
condor_status -l -any -constraint 'MyType == "Defrag"'
\end{verbatim}

\index{SMP machines!configuration|)}
\index{multi-core machines!configuration|)}
