The following manual provides both an overview of the installation, deployment
and use of quill and also a description of various aspects of quill that are
significant to deployment.

Here's a short version of the steps needed to install and use quill.  The 
rest of the document simply elaborates on each step:

1) Install postgres server and client libraries if they aren't already
2) Configure postgres to suit quill
3) Unpack and build the quill server and client tools
4) Modify condor_config with quill related options
5) Invoke the quill daemon and start querying it
6) Miscellaneous issues (database schema, security, etc.)

Quill consists of three components: a) the quill server which maintains 
the job queue and history tables in the database, b) a modified condor_q 
tool called condor_q++, which can be used to query the job queue tables, 
and c) a modified condor_history tool called condor_history++ .  As part 
of the official condor release, these two query tools may replace their 
traditional counterparts as in addition to querying the database, these 
modified tools allows users access to the old features of querying the 
schedd and the history file respectively.  However, in this README file, 
I will continue to use condor_q++ and condor_history++ to refer to the 
modified query tools.

1) Quill uses the postgres server as its backend and the postgres client 
library, libpq to talk to the server. While the 8.0 version of postgres
will be included as part of condor externals, quill has also been tested with 
earlier versions of postgres (specifically 7.4).

In addition, one can obtain the postgres source from:

http://www.postgresql.org/ftp/source/

Installation instructions are detailed in:
http://www.postgresql.org/docs/8.0/static/installation.html

2) The following steps need to be taken after postgres is installed.  These
are done only once; quill takes care of all other database creation and
maintenance tasks.

a) The quill daemon and client tools connect to the database as users 
'quillreader' and 'quillwriter' respectively.  We're talking about 
database users and not operating system users (two completely different 
things).  So if those users dont already exist, they need to be added 
using the 'createuser' command in the postgres bin directory.  Moreover they 
need to be assigned appropriate passwords; these passwords will be used by the
quill tools to connect to the database in a secure way.  User quillreader 
should not be allowed to create more databases nor create more users.  
User quillwriter should also not be allowed to create more users however 
it should be allowed to create more databases.  The following commands 
creates the two users with the appropriate permissions (be ready to enter 
the corresponding passwords when prompted):

/path/to/postgres/bin/directory/createuser quillreader --no-createdb --no-adduser --pwprompt
/path/to/postgres/bin/directory/createuser quillwriter --createdb --no-adduser --pwprompt

b) Postgres should be configured to accept tcp/ip connections.  
In version 7, this was done by setting tcpip_socket=true in the 
postgresql.conf file.  In version 8, this has changed.  Look at the 
listen_addresses variable in the same file.  Set yours appropriately,
e.g. listen_addresses = '*' (here, '*' means any ip interface)

c) Postgres needs to be configured to accept tcp/ip connections from 
certain hosts.  This also enables remote connections.  This is done inthe 
pg_hba.conf file which usually resides in the postgres server's data 
directory.  While the particular syntax and semantics for host based 
configuration can vary from site to site, basically one needs to allow 
access to any hosts that will access this database server, either by way 
of the quill daemon itself writing to the server, or by way of the 
condor_q++ tool querying this server.  For example, in order to give 
database users 'quillreader' and 'quillwriter' password-enabled access to all 
databases on current machine from any other machine in the network add the 
following:

host    all       quillreader       128.105.0.0       255.255.0.0    password
host    all       quillwriter       128.105.0.0       255.255.0.0    password

Note that in addition to the database specified by DATABASE_NAME in the
condor_config file, the quill daemon also needs access to the database
'template1'.  This is because in order to create the former database in 
the first place, it needs to connect to the latter.

Once the server is up and running and the client libraries are installed, 
we can now go ahead and install quill.  

3) Compiling and linking Quill

Quill has been fully integrated into the condor build system.  This means
that condor_quill appears as a directory under the top level src/ directory
in the condor source and building the condor source will also automatically
build quill and both of its query tools.

4) Modifying condor_config
Now that we have built and installed quill, its time to tweak the 
condor_config file to include quill related options.

The following variables need to either be modified or added:

The first ones are DAEMON_LIST and DC_DAEMON_LIST.  Add QUILL to this list as 
shown below:
DAEMON_LIST                     = MASTER, etc. etc.,  QUILL
DC_DAEMON_LIST                  = MASTER, etc. etc.,  QUILL

Add .quillwritepassword to the VALID_SPOOL_FILES variable, since we 
DONT want condor_preen to delete this file thinking its junk
VALID_SPOOL_FILES	= job_queue.log, etc. etc., .quillwritepassword

We need to tell it where it resides and what are its start-up arguments:
QUILL                           = $(SBIN)/condor_quill
QUILL_ARGS                      = -f

Quill writes to its own log just as the other daemons.  This log can be 
checked to see quill's run-time behavior and any malfunctions.
QUILL_LOG       = $(LOG)/QuillLog

The following options go in the daemon-specific (in this case, quill) 
section with appropriately modified values to suit the local environment:

QUILL_NAME              = some-unique-quill-name.cs.wisc.edu
DATABASE_NAME           = database-for-some-unique-quill-name
DATABASE_IPADDRESS      = <databaseipaddress:port>
# the following parameter's units is in seconds
QUILL_POLLING_PERIOD    = 10
# the following parameter's units is in hours
QUILL_HISTORY_CLEANING_INTERVAL = 24
# the following parameter's units is in days
QUILL_HISTORY_DURATION 	= 30
QUILL_IS_REMOTELY_QUERYABLE = 1
QUILL_QUERY_PASSWORD 	=  password-for-database-user-quillreader
QUILL_ADDRESS_FILE      = $(LOG)/.quill_address

Following is a description on each.  Skip to the next section for a brief 
overview on how to query quill:

QUILL_NAME: 
This is the name of this quill server.  Each quill server sends an ad to 
the collector containing its name.  As such its important that the name of 
a quill server should not conflict with that of any other quill server, or 
for that matter, any schedd.  The latter is because each quill sends a 
SCHEDD_AD to the collector, and as such, from the perspective of the 
collector, a quill is just another schedd. It might be convenient to simply 
name the quill server, quill-machinename.fully.qualified.address 

DATABASE_NAME and DATABASE_IPADDRESS
These two variables are used to determine the location of the database server 
that this quill would talk to, and the name of the database that it creates.  
The database port should correspond to that specified in the postgresql 
configuration file.  

More than one quill server can talk to the same database 
server.  This can be done by simply letting all the DATABASE_IPADDRESS 
point to the same database server.
IMPORTANT:  If more than one quill server are sharing the same database 
server, then the DATABASE_NAME variable for all of them should be unique.  
Otherwise, there would be all kinds of bizarre overwriting.

QUILL_POLLING_PERIOD
This controls the frequency with which quill polls the job_queue.log file.  
By default, it is 10 seconds.  Since quill works by periodically sniffing 
the log file for updates and then sending those updates to the database, 
this variable controls the tradeoff between the currency of query results 
and quill's load on the system - usually negligible.  

QUILL_HISTORY_CLEANING_INTERVAL and QUILL_HISTORY_DURATION 
These two variables control the deletion of historical jobs from the 
database.  QUILL_HISTORY_DURATION is the number of days after completion 
(more precisely, the number of days since the history ad got into the 
history database - those two might be different if a job is completed but 
stays in the queue for a while) that a given job will stay in the 
database.  So all jobs beyond QUILL_HISTORY_DURATION will be deleted.  
Now, scanning the entire database for old jobs can get pretty expensive, 
so the other variable QUILL_HISTORY_CLEANING_INTERVAL is the number of 
hours between two successive scans.  By default, QUILL_HISTORY_DURATION is 
set to 180 days and QUILL_HISTORY_CLEANING_INTERVAL is set to 24 hours.

QUILL_IS_REMOTELY_QUERYABLE
Thanks to postgres one can now remotely query both the job queue and the
history tables. This variable controls whether this remote querying 
feature should be enabled.  By default it is 1 (true).  Note that even if 
this is 0 (false), one can still query a remote schedd using 
	condor_q++ -name remote-schedd-name
This variable only controls whether the database tables controlled by this 
particular quill server is remotely queryable via
	condor_q++ -name remote-quill-name

QUILL_QUERY_PASSWORD
In order for the query tools to connect to a database, it needs to provide
the password that is assigned to database user 'quillreader' in step 2c) 
above. This variable is then advertised by the quill daemon to the collector.
This facility enables remote querying: remote condor_q++ query tools 
first ask the collector for the password associated with a particular quill
database and then query that database.  Users who do not have access to the 
collector cannot view the password and as such cannot query the database.
Again, this password just provides 'read' access to the database.  

QUILL_ADDRESS_FILE      = $(LOG)/.quill_address
When quill starts up, it can place it's address (IP and port)
into a file.  This way, tools running on the local machine don't
need to query the central manager to find quill.  This 
feature can be turned off by commenting out the variable.

5) Invoking the quill daemon and querying it
Once the condor_config file is updated with the above arguments, the    
quill daemon can be started by either restarting condor using 
condor_restart or just starting it using condor_master.  All the daemons 
in the DAEMON_LIST variable, as updated above, are started and managed by 
the master accordingly.  

The quill daemon is responsible for maintaining a database mirror of the 
job_queue and history logs.  One can query those two using condor_q++ and 
condor_history++ respectively.  Both these two tools retain all their old 
functionality, i.e. condor_q++ can be used to query the schedd and 
condor_history++ can be used to query the history file, just like 
condor_q and condor_history respectively.  Moreover, they retain all 
their old options plus some more thanks to database technology.  For 
example, as before, we can query both using the job id (cluster.proc), 
owner, dags, io, cputime, etc.  Orthogonally, just as how we could query 
remote schedds, we can also query remote quill databases for job queue 
and historical information.  The latter is new functionality thanks to 
the remote querying functionality in Postgres.  

The -help option can be used to look at all the options supported by both 
tools.

Examples:

a) Query a remote quill daemon on regular.cs.wisc.edu for all the jobs in 
the queue
	condor_q++ -name quill-regular.cs.wisc.edu
Note that the parameter after -name is the same as that specified for the 
QUILL_NAME variable in part 3) above.  Also, the ip address and port of 
the database server hosting the data of this particular remote quill daemon 
can be figured out by the DATABASE_IPADDRESS and DATABASE_NAME variables
specified in the SCHEDD_AD sent by the quill daemon to the collector.


b) Query a remote quill daemon on regular.cs.wisc.edu for all historical 
jobs belonging to owner 'akini'.
	condor_history++ -name quill-regular.cs.wisc.edu akini

c) Query the local quill daemon for the average time spent in the queue 
for all non-completed jobs. 
	condor_q++ -avgqueuetime 
This is a new query.  -avgqueuetime is defined as the average of 
(currenttime - jobsubmissiontime) over all jobs which are neither 
completed (JobStatus == 4) or removed (JobStatus == 3).

d) Query the local quill daemon for all historical jobs completed since Apr 1, 
2005 at 13h 00m.
	condor_history++ -completedsince '04/01/2005 13:00'
This is also a new query.  It fetches all jobs which got into the 
'Completed' state on or after the specified timestamp.  We follow 
Postgres's date/time syntax rules as it encompasses most format options.  
See http://www.postgresql.org/docs/8.0/static/datatype-datetime.html#AEN4516
for the various timestamp formats.

e) Query the local schedd for all the jobs in the queue submitted by akini
	condor_q++ -name akini@karadi.cs.wisc.edu
This is using the old way of querying the schedd.  Since now, the default 
is to query the database, we need to explicitly tell condor_q++ to query
the local schedd by using the -name flag.

6) Miscellaneous Issues

a) Database Schema

With only 7 tables and 2 views, quill uses a relatively simple database schema.
These can be broadly divided into tables used to store job queue information 
and those used to store historical information.  

The job queue part of the schema closely follows condor's classad data model, 
i.e. each row in these tables describe an <attribute,value> pair of the classad.
Additionally, just as how condor distinguishes a ClusterAd from a ProcAd where 
the former stores attributes common to all jobs within a cluster whereas the 
latter stores attributes specific to each job, the schema also makes this 
distinction.  Finally, numerical and string valued attributes are stored 
separately.

Thus we have four tables

i)   ClusterAds_Str (cid int, attr text, val text, primary key (cid, attr))
ii)  ClusterAds_Num (cid int, attr text, val double precision, primary key (cid, attr))
iii) ProcAds_Str (cid int, pid int, attr text, val text, primary key (cid, pid, attr))
iv)  ProcAds_Num (cid int, pid int, attr text, val double precision, primary key (cid, pid, attr))

In addition to the <attribute, value>, each row contains the cluster-id (cid) 
and in the case of the ProcAd tables, also the proc-id (pid).  

Since each classad would be split into potentially two tables (string and 
numeric), there are views that unify them into a single entity in order to 
simplify queries. 

Here are the view definitions:

v)   Definition of ClusterAds view
     CREATE VIEW ClusterAds as
        select cid, attr, val from ClusterAds_Str UNION ALL
        select cid, attr, cast(val as text) from ClusterAds_Num;


vi)  Definition of ProcAds view
     CREATE VIEW ProcAds as
        select cid, pid, attr, val from ProcAds_Str UNION ALL
        select cid, pid, attr, cast(val as text) from ProcAds_Num;

Finally, the job queue part of the schema also contains a table that stores 
metadata information related to the job_queue.log file.

vii) JobQueuePollingInfo (
        last_file_mtime         BIGINT,
        last_file_size          BIGINT,
        last_next_cmd_offset    BIGINT,
        last_cmd_offset         BIGINT,
        last_cmd_type           SMALLINT,
        last_cmd_key            text,
        last_cmd_mytype         text,
        last_cmd_targettype     text,
        last_cmd_name           text,
        last_cmd_value          text)
	
At all times, there's only 1 row in this table and it describes information 
related to the last time quill polled the job_queue.log file. 

last_file_mtime and last_file_size - the last modified time and size of the file
last_cmd_offset and last_next_cmd_offset - The offsets of the record last read 
	from the file and its successive record.
last_cmd_type - the command type (101, 102, etc.) of the record
last_cmd_key, last_cmd_mytype, last_cmd_targettype, last_cmd_name, last_cmd_value - 
	together, these attributes define the record itself.  The key refers 
	to the combined "cid.pid" pair, mytype and target usually contains 
	Job and Machine respectively, and finally the name and value 
	contains the <attribute,value> pair.

The historical information on the other hand is slightly differently designed.
Instead of a purely vertical data model (each row is a <attribute,value> pair)
, we have two tables that together represent the complete job classad. 
Their schema is as follows:

viii) History_Horizontal(
        cid                  int,
        pid                  int,
        Owner                text,
        QDate                int,
        RemoteWallClockTime  int,
        RemoteUserCpu        float,
        RemoteSysCpu         float,
        ImageSize            int,
        JobStatus            int,
        JobPrio              int,
        Cmd                  text,
        CompletionDate       int,
        LastRemoteHost       text,
        primary key(cid,pid))

and 

ix)  History_Vertical (cid int, pid int, attr text, val text, primary key (cid, pid, attr))

Each historical job ad is divided into its horizontal and vertical 
counterparts.  This division was made because of query performance 
reasons.  While its easier to store classads in a vertical table, queries 
on vertical tables generally perform worse than those on horizontal tables 
since the latter has lot fewer records.  However, in Condor, since job ads 
dont have a fixed schema (users can define their own attributes), a purely 
horizontal schema would end up having a lot of null values. As such, we 
have a hybrid schema where attributes on which queries are frequently 
performed (via condor_history++) are put in the History_Horizontal table 
and the other attributes are stored vertically (just as in the 
Cluster/Proc tables above) in the History_Vertical table. Also 
History_Horizontal contains all the attributes needed to service the short 
form of the condor_history++ command (i.e. without the -l option). The 
resulting hybrid schema has proven to be the most efficient in servicing 
condor_history++ queries.  The job queue tables (Cluster and Proc) were 
not designed in this hybrid manner because job queues aren't as large as 
history; just a vertical schema worked great.

b) Security in Quill

There are several layers of security in Quill, some provided by condor and 
others provided by the database.  Firstly, all accesses to the database 
are password-protected.  

i) As mentioned in section 2c) above, the query tools, condor_q++ and 
condor_history++ connect to the database as user 'quillreader'.  The 
password for this user can vary from one database to another and as such, 
each quill daemon advertises this password to the collector.  The query 
tools then obtain this password from the collector and connect 
successfully to the database.  Access to the database by the 'quillreader' 
user is read-only as this is sufficient for the query tools.  The quill 
daemon ensures this protected access using the sql GRANT command when it 
first creates the tables in the database.  Note that access to 
quillreader's password itself can be blocked by blocking access to the 
collector, a feature already supported in Condor.

ii) The quill daemon, on the other hand, needs read and write access to the 
database.  As such, it connects as user 'quillwriter' who has owner 
priviledges to the database.  Since this gives all access to the 
'quillwriter' user, its password cannot be stored in a public place (such 
as the collector).  For this reason, quillwriter's password is stored in a 
file called .quillwritepassword in the condor spool directory. Appropriate 
read/write protections on this file guarantee secure access to the database.
This file must be created and protected by the site administrator;  if 
this file does not exist as and where expected, the quill daemon logs and 
error and exits.

iii) Finally, as mentioned in section 2c) above, the IsRemotelyQueryable 
attribute in the SCHEDD_AD advertised by the quill daemon to the collector 
can be used by site administrators to disallow the database from being 
read by all remote condor query tools.

c) Maintenance (Non) Issues

There are virtually no maintenance issues in Quill.  Once started, it 
checks if all necessary database related structures (database itself, 
tables, indices, views) are present and creates them if they are not 
present. It also purges old historical jobs based on user policy (see 
section 4 above) and garbage collection in the database (using the 
postgres VACUUM ANALYZE command). Of course, if Quill is shut down and the 
database is no longer needed, it can be dropped using the postgres dropdb 
command.  
