{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Submitting and Managing Jobs\n",
    "\n",
    "Launch this tutorial in a Jupyter Notebook on Binder: \n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/htcondor/htcondor-python-bindings-tutorials/master?urlpath=lab/tree/users/Submitting-and-Managing-Jobs.ipynb)\n",
    "\n",
    "In this module, we will learn how to submit and manage jobs from Python. \n",
    "We will learn how to submit jobs with various toy executables, how to ask HTCondor for information about them, and how to tell HTCondor to do things with them.\n",
    "\n",
    "We start by importing the relevant modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import htcondor  # for submitting jobs, querying HTCondor, etc.\n",
    "import classad   # ClassAds are HTCondor's internal data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Submitting a Simple Job\n",
    "\n",
    "To submit a job, we must first describe it.\n",
    "A submit description is held in a `Submit` object.\n",
    "`Submit` objects consist of key-value pairs, and generally behave like Python dictionaries.\n",
    "If you're familiar with HTCondor's submit file syntax, you should think of each line in the submit file as a single key-value pair in the `Submit` object.\n",
    "\n",
    "Let's start by writing a `Submit` object that describes a job that executes the `hostname` command on an execute node, which prints out the \"name\" of the node.\n",
    "Since `hostname` prints its results to standard output (stdout), we will capture stdout and bring it back to the submit machine so we can see the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executable = /bin/hostname\n",
      "output = hostname.out\n",
      "error = hostname.err\n",
      "log = hostname.log\n",
      "request_cpus = 1\n",
      "request_memory = 128MB\n",
      "request_disk = 128MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hostname_job = htcondor.Submit({\n",
    "    \"executable\": \"/bin/hostname\",  # the program to run on the execute node\n",
    "    \"output\": \"hostname.out\",       # anything the job prints to standard output will end up in this file\n",
    "    \"error\": \"hostname.err\",        # anything the job prints to standard error will end up in this file\n",
    "    \"log\": \"hostname.log\",          # this file will contain a record of what happened to the job\n",
    "    \"request_cpus\": \"1\",            # how many CPU cores we want\n",
    "    \"request_memory\": \"128MB\",      # how much memory we want\n",
    "    \"request_disk\": \"128MB\",        # how much disk space we want\n",
    "})\n",
    "\n",
    "print(hostname_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The available descriptors are documented in the `condor_submit` [manual](https://htcondor.readthedocs.io/en/latest/man-pages/condor_submit.html).\n",
    "\n",
    "Note that we gave it several relative filepaths.\n",
    "These paths are relative to the directory containing this Jupyter notebook (or, more generally, the current working directory).\n",
    "When we run the job, you should see those files appear in the file browser on the left as HTCondor creates them.\n",
    "\n",
    "Now that we have a description, let's submit a job.\n",
    "To do so, we must ask the HTCondor scheduler to open a transaction.\n",
    "Once we have the transaction, we can \"queue\" a job via the `Submit` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "schedd = htcondor.Schedd()          # get the Python representation of the scheduler\n",
    "with schedd.transaction() as txn:   # open a transaction, represented by `txn`\n",
    "    cluster_id = hostname_job.queue(txn)     # queues one job in the current transaction; returns job's ClusterID\n",
    "    \n",
    "print(cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number returned by the `queue` method is the `ClusterID` for the submission.\n",
    "It uniquely identifies this submission.\n",
    "Later in this module, we will use it to ask the scheduler for information about our jobs.\n",
    "\n",
    "It isn't important to understand the transaction mechanics for now; think of it as boilerplate.\n",
    "(There are advanced use cases where it might be useful.)\n",
    "\n",
    "For now, our job will hopefully have finished running.\n",
    "You should be able to see the files in the file browser on the left.\n",
    "Try opening one of them and seeing what's inside.\n",
    "\n",
    "We can also look at the output from inside Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e389f64a7811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('hostname.out', mode = 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file doesn't exist for some reason, it means your job didn't run.\n",
    "If you got some text, it worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Multiple Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "By default, each `queue` will submit a single job.\n",
    "A more common use case is to submit many jobs at once, often sharing some base submit description.\n",
    "Let's write a new submit description which runs `sleep`.\n",
    "\n",
    "When we have multiple **jobs** in a single **cluster**, each job will be identified not just by its **ClusterID** but also by a **ProcID**.\n",
    "We can use the ProcID to separate the output and error files for each individual job.\n",
    "Anything that looks like `$(...)` in a submit description is a **macro**, which will be expanded later by HTCondor.\n",
    "The ProcID expands to a series of incrementing integers, starting at 0.\n",
    "So the first job in a cluster will have ProcID 0, the next will have ProcID 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executable = /bin/sleep\n",
      "arguments = 1m\n",
      "output = sleep-$(ProcID).out\n",
      "error = sleep-$(ProcID).err\n",
      "log = sleep.log\n",
      "request_cpus = 1\n",
      "request_memory = 128MB\n",
      "request_disk = 128MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sleep_job = htcondor.Submit({\n",
    "    \"executable\": \"/bin/sleep\",      \n",
    "    \"arguments\": \"1m\",                # sleep for 1 minute\n",
    "    \"output\": \"sleep-$(ProcID).out\",  # output and error separated by job, using the $(ProcID) macro\n",
    "    \"error\": \"sleep-$(ProcID).err\",  \n",
    "    \"log\": \"sleep.log\",               # we send all of the HTCondor logs for every individual job to the same file still (not split up!)\n",
    "    \"request_cpus\": \"1\",             \n",
    "    \"request_memory\": \"128MB\",       \n",
    "    \"request_disk\": \"128MB\",           \n",
    "})\n",
    "\n",
    "print(sleep_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will submit 10 of these jobs.\n",
    "All we need to change from our previous `queue` call is to add the `count` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "schedd = htcondor.Schedd()                \n",
    "with schedd.transaction() as txn:       \n",
    "    cluster_id = sleep_job.queue(txn, count=10)  # submit 10 jobs\n",
    "print(cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a bunch of jobs in flight, we might want to check how they're doing.\n",
    "We can ask the scheduler about jobs by using its `query` method.\n",
    "We give it a **constraint**, which tells it which jobs to look for, and a **projection** (called ``attr_list`` for historical reasons), which tells it what information to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ ClusterId = 2; ProcId = 0; JobStatus = 1; Out = \"sleep-0.out\"; ServerTime = 1582304153 ],\n",
       " [ ClusterId = 2; ProcId = 1; JobStatus = 1; Out = \"sleep-1.out\"; ServerTime = 1582304153 ],\n",
       " [ ClusterId = 2; ProcId = 2; JobStatus = 1; Out = \"sleep-2.out\"; ServerTime = 1582304153 ],\n",
       " [ ClusterId = 2; ProcId = 3; JobStatus = 1; Out = \"sleep-3.out\"; ServerTime = 1582304153 ],\n",
       " [ ClusterId = 2; ProcId = 4; JobStatus = 1; Out = \"sleep-4.out\"; ServerTime = 1582304153 ],\n",
       " [ ClusterId = 2; ProcId = 5; JobStatus = 1; Out = \"sleep-5.out\"; ServerTime = 1582304153 ],\n",
       " [ ClusterId = 2; ProcId = 6; JobStatus = 1; Out = \"sleep-6.out\"; ServerTime = 1582304153 ],\n",
       " [ ClusterId = 2; ProcId = 7; JobStatus = 1; Out = \"sleep-7.out\"; ServerTime = 1582304153 ],\n",
       " [ ClusterId = 2; ProcId = 8; JobStatus = 1; Out = \"sleep-8.out\"; ServerTime = 1582304153 ],\n",
       " [ ClusterId = 2; ProcId = 9; JobStatus = 1; Out = \"sleep-9.out\"; ServerTime = 1582304153 ]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedd.query(\n",
    "    constraint='ClusterId=={}'.format(cluster_id),\n",
    "    attr_list=[\"ClusterId\", \"ProcId\", \"JobStatus\", \"Out\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to notice here:\n",
    "- Depending on how long it took you to run the cell, you may only get a few of your 10 jobs in the query. Jobs that have finished **leave the queue**, and will no longer show up in queries. To see those jobs, you must use the `history` method instead, which behaves like `query`, but **only** looks at jobs that have left the queue.\n",
    "- The results most likely did not come back in ProcID-sorted order. If you want to order the results, you must do so yourself.\n",
    "- Attributes are often renamed between the submit description and the actual job description in the queue. See [the manual](https://htcondor.readthedocs.io/en/latest/classad-attributes/job-classad-attributes.html) for a description of the job attribute names.\n",
    "- The objects returned by the query are instances of `ClassAd`. ClassAds are the common data exchange format used by HTCondor. In Python, they mostly behave like dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Itemdata to Vary Over Parameters\n",
    "\n",
    "By varying some part of the submit description using the ProcID, we can change how each individual job behaves.\n",
    "Perhaps it will use a different input file, or a different argument.\n",
    "However, we often want more flexibility than that.\n",
    "Perhaps our input files are named after different cities, or by timestamp, or whatever other naming scheme already exists.\n",
    "\n",
    "To use such information in the submit description, we need to use **itemdata**.\n",
    "Itemdata lets us pass arbitrary extra information when we queue, which we can reference with macros inside the submit description.\n",
    "This lets use the full power of Python to generate the submit descriptions for our jobs.\n",
    "\n",
    "Let's mock this situation out by generating some files with randomly-chosen names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "def random_string(length):\n",
    "    \"\"\"Produce a random lowercase ASCII string with the given length.\"\"\"\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k = length))\n",
    "\n",
    "# make a directory to hold the input files; don't worry about this code\n",
    "input_dir = pathlib.Path.cwd() / \"inputs\"\n",
    "shutil.rmtree(input_dir, ignore_errors = True)\n",
    "input_dir.mkdir()\n",
    "\n",
    "# make 10 input files\n",
    "for idx in range(10):\n",
    "    input_file = input_dir / \"{}.txt\".format(random_string(5))\n",
    "    input_file.write_text(\"Hello from job {}\".format(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get a list of all the files in the input directory, for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/tutorials/users/inputs/fueto.txt\n",
      "/home/jovyan/tutorials/users/inputs/brmdj.txt\n",
      "/home/jovyan/tutorials/users/inputs/fkaci.txt\n",
      "/home/jovyan/tutorials/users/inputs/bswro.txt\n",
      "/home/jovyan/tutorials/users/inputs/rhbnh.txt\n",
      "/home/jovyan/tutorials/users/inputs/zolpn.txt\n",
      "/home/jovyan/tutorials/users/inputs/lwokt.txt\n",
      "/home/jovyan/tutorials/users/inputs/hrrwi.txt\n",
      "/home/jovyan/tutorials/users/inputs/lmtve.txt\n",
      "/home/jovyan/tutorials/users/inputs/pyfvl.txt\n"
     ]
    }
   ],
   "source": [
    "input_files = [path.as_posix() for path in input_dir.iterdir()]\n",
    "\n",
    "for path in input_files:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make our submit description.\n",
    "Our goal is just to print out the text held in each file, which we can do using `cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executable = /bin/cat\n",
      "arguments = $(input_file)\n",
      "transfer_input_files = $(input_file)\n",
      "output = cat-$(ProcID).out\n",
      "error = cat-$(ProcID).err\n",
      "log = cat.log\n",
      "request_cpus = 1\n",
      "request_memory = 128MB\n",
      "request_disk = 128MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_job = htcondor.Submit({\n",
    "    \"executable\": \"/bin/cat\",      \n",
    "    \"arguments\": \"$(input_file)\",             # we will pass in the value for this macro via itemdata\n",
    "    \"transfer_input_files\": \"$(input_file)\",  # we also need HTCondor to move the file to the execute node\n",
    "    \"output\": \"cat-$(ProcID).out\",  \n",
    "    \"error\": \"cat-$(ProcID).err\",  \n",
    "    \"log\": \"cat.log\",              \n",
    "    \"request_cpus\": \"1\",             \n",
    "    \"request_memory\": \"128MB\",       \n",
    "    \"request_disk\": \"128MB\",           \n",
    "})\n",
    "\n",
    "print(cat_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The itemdata should be passed as a list of dictionaries, where the keys are the macro names to replace in the submit description.\n",
    "In our case, the key should be `input_file`, and we should have a list of 10 dictionaries, each with one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_file': '/home/jovyan/tutorials/users/inputs/fueto.txt'}\n",
      "{'input_file': '/home/jovyan/tutorials/users/inputs/brmdj.txt'}\n",
      "{'input_file': '/home/jovyan/tutorials/users/inputs/fkaci.txt'}\n",
      "{'input_file': '/home/jovyan/tutorials/users/inputs/bswro.txt'}\n",
      "{'input_file': '/home/jovyan/tutorials/users/inputs/rhbnh.txt'}\n",
      "{'input_file': '/home/jovyan/tutorials/users/inputs/zolpn.txt'}\n",
      "{'input_file': '/home/jovyan/tutorials/users/inputs/lwokt.txt'}\n",
      "{'input_file': '/home/jovyan/tutorials/users/inputs/hrrwi.txt'}\n",
      "{'input_file': '/home/jovyan/tutorials/users/inputs/lmtve.txt'}\n",
      "{'input_file': '/home/jovyan/tutorials/users/inputs/pyfvl.txt'}\n"
     ]
    }
   ],
   "source": [
    "itemdata = [{'input_file': path} for path in input_files]\n",
    "\n",
    "for item in itemdata:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll submit the jobs, using `queue_with_itemdata` instead of `queue`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "schedd = htcondor.Schedd()                \n",
    "with schedd.transaction() as txn:         \n",
    "    submit_result = cat_job.queue_with_itemdata(txn, itemdata = iter(itemdata))  # submit one job for each item in the itemdata\n",
    "    \n",
    "print(submit_result.cluster())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `queue_with_itemdata` returns a \"submit result\", not just the ClusterID.\n",
    "The ClusterID can be retreived from the submit result with its `cluster()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Managing Jobs\n",
    "\n",
    "Once a job is in queue, the scheduler will try its best to execute it to completion. \n",
    "There are several cases where you may want to interrupt the normal flow of jobs. \n",
    "Perhaps the results are no longer needed; perhaps the job needs to be edited to correct a submission error. \n",
    "These actions fall under the purview of **job management**.\n",
    "\n",
    "There are two `Schedd` methods dedicated to job management:\n",
    "\n",
    "* `edit()`: Change an attribute for a set of jobs.\n",
    "* `act()`: Change the state of a job (remove it ffrom the queue, hold it, suspend it, etc.).\n",
    "\n",
    "The `act` method takes an argument from the `JobAction` enum.\n",
    "Commonly-used values include:\n",
    "\n",
    "* `Hold`: put a job on hold, vacating a running job if necessary.  A job will stay in the hold\n",
    "   state until told otherwise.\n",
    "* `Release`: Release a job from the hold state, returning it to Idle.\n",
    "* `Remove`: Remove a job from the queue. If it is running, it will stop running.\n",
    "   This requires the execute node to acknowledge it has successfully vacated the job, so ``Remove`` may\n",
    "   not be instantaneous.\n",
    "* `Vacate`: Cause a running job to be killed on the remote resource and return to the Idle state.  With\n",
    "  `Vacate`, jobs may be given significant time to cleanly shut down.\n",
    "\n",
    "To play with this, let's bring back our sleep submit description, but increase the sleep time significantly so that we have time to interact with the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executable = /bin/sleep\n",
      "arguments = 10m\n",
      "output = sleep-$(ProcID).out\n",
      "error = sleep-$(ProcID).err\n",
      "log = sleep.log\n",
      "request_cpus = 1\n",
      "request_memory = 128MB\n",
      "request_disk = 128MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "long_sleep_job = htcondor.Submit({\n",
    "    \"executable\": \"/bin/sleep\",      \n",
    "    \"arguments\": \"10m\",                # sleep for 10 minutes\n",
    "    \"output\": \"sleep-$(ProcID).out\", \n",
    "    \"error\": \"sleep-$(ProcID).err\",  \n",
    "    \"log\": \"sleep.log\", \n",
    "    \"request_cpus\": \"1\",             \n",
    "    \"request_memory\": \"128MB\",       \n",
    "    \"request_disk\": \"128MB\",           \n",
    "})\n",
    "\n",
    "print(long_sleep_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "schedd = htcondor.Schedd()\n",
    "with schedd.transaction() as txn:\n",
    "    cluster_id = long_sleep_job.queue(txn, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an experiment, let's set an arbitrary attribute on the jobs and check that it worked.\n",
    "When we're really working, we could do things like change the amount of memory a job has requested by editing its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ ClusterId = 4; ProcId = 0; foo = \"bar\"; JobStatus = 1; ServerTime = 1582304171 ],\n",
       " [ ClusterId = 4; ProcId = 1; foo = \"bar\"; JobStatus = 1; ServerTime = 1582304171 ],\n",
       " [ ClusterId = 4; ProcId = 2; foo = \"bar\"; JobStatus = 1; ServerTime = 1582304171 ],\n",
       " [ ClusterId = 4; ProcId = 3; foo = \"bar\"; JobStatus = 1; ServerTime = 1582304171 ],\n",
       " [ ClusterId = 4; ProcId = 4; foo = \"bar\"; JobStatus = 1; ServerTime = 1582304171 ]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sets attribute foo to the string \"bar\" for all of our jobs\n",
    "schedd.edit(\"ClusterID == {}\".format(cluster_id), \"foo\", '\"bar\"')\n",
    "\n",
    "# do a query to check the value of attribute foo\n",
    "schedd.query(\n",
    "    constraint='ClusterId == {}'.format(cluster_id),\n",
    "    attr_list=[\"ClusterId\", \"ProcId\", \"JobStatus\", \"foo\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the job status appears to be an attribute, we cannot `edit` it directly.\n",
    "As mentioned above, we must instead `act` on the job.\n",
    "Let's hold the first two jobs so that they stop running, but leave the others going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ ClusterId = 4; ProcId = 0; JobStatus = 5; ServerTime = 1582304172 ],\n",
       " [ ClusterId = 4; ProcId = 1; JobStatus = 5; ServerTime = 1582304172 ],\n",
       " [ ClusterId = 4; ProcId = 2; JobStatus = 1; ServerTime = 1582304172 ],\n",
       " [ ClusterId = 4; ProcId = 3; JobStatus = 1; ServerTime = 1582304172 ],\n",
       " [ ClusterId = 4; ProcId = 4; JobStatus = 1; ServerTime = 1582304172 ]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hold the first two jobs\n",
    "schedd.act(htcondor.JobAction.Hold, \"ClusterID=={} && ProcID <= 1\".format(cluster_id))\n",
    "\n",
    "# check the status of the jobs\n",
    "schedd.query(\n",
    "    constraint='ClusterId == {}'.format(cluster_id),\n",
    "    attr_list=[\"ClusterId\", \"ProcId\", \"JobStatus\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various job statuses are represented by numbers. `1` means `Idle`, `2` means `Running`, and `5` means `Held`. If you see some `JobStatus = 5` above, then we succeeded!\n",
    "\n",
    "The opposite of `JobAction.Hold` is `JobAction.Release`.\n",
    "Let's release those jobs and let them go back to `Idle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ ClusterId = 4; ProcId = 0; JobStatus = 1; ServerTime = 1582304173 ],\n",
       " [ ClusterId = 4; ProcId = 1; JobStatus = 1; ServerTime = 1582304173 ],\n",
       " [ ClusterId = 4; ProcId = 2; JobStatus = 2; ServerTime = 1582304173 ],\n",
       " [ ClusterId = 4; ProcId = 3; JobStatus = 2; ServerTime = 1582304173 ],\n",
       " [ ClusterId = 4; ProcId = 4; JobStatus = 1; ServerTime = 1582304173 ]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedd.act(htcondor.JobAction.Release, \"ClusterID=={}\".format(cluster_id))\n",
    "\n",
    "schedd.query(\n",
    "    constraint='ClusterId == {}'.format(cluster_id),\n",
    "    attr_list=[\"ClusterId\", \"ProcId\", \"JobStatus\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we simply released all the jobs in the cluster. Releasing a job that is not held doesn't do anything, so we don't have to be extremely careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Now let's practice what we've learned.\n",
    "\n",
    "- In each exercise, you will be given a piece of code and a test that does not yet pass.\n",
    "- Modify the code, or add new code to it, to pass the test.\n",
    "- You can run the test by running the block it is in.\n",
    "- Feel free to look at the test for clues as to how to modify the code.\n",
    "- Many of the exercises can be solved either by using Python to generate inputs, or by using advanced features of the [ClassAd language](https://htcondor.readthedocs.io/en/latest/misc-concepts/classad-mechanism.html#htcondor-s-classad-mechanism). Either way is valid!\n",
    "- Don't modify the test. That's cheating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Incrementing Sleeps\n",
    "\n",
    "Submit five jobs which sleep for `5`, `6`, `7`, `8`, and `9` seconds, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY OR ADD TO THIS BLOCK...\n",
    "\n",
    "incrementing_sleep = htcondor.Submit({\n",
    "    \"executable\": \"/bin/sleep\",      \n",
    "    \"arguments\": \"1\",\n",
    "    \"output\": \"ex1-$(ProcID).out\",\n",
    "    \"error\": \"ex1-$(ProcID).err\",  \n",
    "    \"log\": \"ex1.log\",\n",
    "    \"request_cpus\": \"1\",\n",
    "    \"request_memory\": \"128MB\",\n",
    "    \"request_disk\": \"128MB\",\n",
    "})\n",
    "\n",
    "schedd = htcondor.Schedd()\n",
    "with schedd.transaction() as txn:\n",
    "    cluster_id = incrementing_sleep.queue(txn, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... TO MAKE THIS TEST PASS\n",
    "\n",
    "expected = [str(i) for i in range(5, 10)]\n",
    "print('Expected ', expected)\n",
    "\n",
    "ads = schedd.query(\"ClusterID == {}\".format(cluster_id), attr_list = [\"Args\"])\n",
    "arguments = sorted(ad[\"Args\"] for ad in ads)\n",
    "print('Got      ', arguments)\n",
    "\n",
    "assert arguments == expected, \"Arguments were not what we expected!\"\n",
    "print(\"The test passed. Good job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Holding Odds\n",
    "\n",
    "Hold all of the odd-numbered jobs in this large cluster.\n",
    "\n",
    "- Note that the test block **removes all of the jobs you own** when it runs, to prevent these long-running jobs from corrupting other tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY OR ADD TO THIS BLOCK...\n",
    "\n",
    "long_sleep = htcondor.Submit({\n",
    "    \"executable\": \"/bin/sleep\",      \n",
    "    \"arguments\": \"10m\",\n",
    "    \"output\": \"ex2-$(ProcID).out\",\n",
    "    \"error\": \"ex2-$(ProcID).err\",  \n",
    "    \"log\": \"ex2.log\",\n",
    "    \"request_cpus\": \"1\",\n",
    "    \"request_memory\": \"128MB\",\n",
    "    \"request_disk\": \"128MB\", \n",
    "})\n",
    "\n",
    "schedd = htcondor.Schedd()\n",
    "with schedd.transaction() as txn:\n",
    "    cluster_id = long_sleep.queue(txn, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... TO MAKE THIS TEST PASS\n",
    "\n",
    "ads = schedd.query(\"ClusterID == {}\".format(cluster_id), attr_list = [\"ProcID\", \"JobStatus\"])\n",
    "proc_to_status = {int(ad['ProcID']): ad['JobStatus'] for ad in sorted(ads, key = lambda ad: ad['ProcID'])}\n",
    "\n",
    "for proc, status in proc_to_status.items():\n",
    "    print(\"Proc {} has status {}\".format(proc, status))\n",
    "\n",
    "schedd.act(htcondor.JobAction.Remove, 'true')\n",
    "assert len(proc_to_status) == 100, \"Wrong number of jobs (perhaps you need to resubmit them?.\"\n",
    "assert all(status == \"5\" for proc, status in proc_to_status.items() if proc % 2 != 0), \"Not all odd jobs were held.\"\n",
    "assert all(status != \"5\" for proc, status in proc_to_status.items() if proc % 2 == 0), \"An even job was held.\"\n",
    "    \n",
    "print(\"The test passed. Good job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Echo to Target\n",
    "\n",
    "Run a job that makes the text `Echo to Target` appear in the job's standard output file `ex3.out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY OR ADD TO THIS BLOCK...\n",
    "\n",
    "echo = htcondor.Submit({\n",
    "    \"output\": \"ex3.out\",\n",
    "    \"error\": \"ex3.err\",  \n",
    "    \"log\": \"ex3.log\",\n",
    "    \"request_cpus\": \"1\",\n",
    "    \"request_memory\": \"128MB\",\n",
    "    \"request_disk\": \"128MB\", \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... TO MAKE THIS TEST PASS\n",
    "\n",
    "import os.path\n",
    "\n",
    "does_file_exist = os.path.exists('ex3.out')\n",
    "assert does_file_exist, \"ex3.txt does not exist!\"\n",
    "\n",
    "expected = 'Echo to Target'\n",
    "print('Expected ', expected)\n",
    "\n",
    "contents = open('ex3.out', mode = 'r').read().strip()\n",
    "print('Got      ', contents)\n",
    "\n",
    "assert expected in contents, \"Contents were not what we expected!\"\n",
    "\n",
    "print(\"The test passed. Good job!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
