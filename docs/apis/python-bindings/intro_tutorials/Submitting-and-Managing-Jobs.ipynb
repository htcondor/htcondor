{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Submitting and Managing Jobs\n",
    "\n",
    "The two most common HTCondor command line tools are `condor_q` and `condor_submit`; in the previous module, we learning the `xquery()` method that corresponds to `condor_q`. Here, we will learn the Python binding equivalent of `condor_submit`.\n",
    "\n",
    "As usual, we start by importing the relevant modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import htcondor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Submitting Jobs\n",
    "---------------\n",
    "\n",
    "We will submit jobs utilizing the dedicated `Submit` object.\n",
    "\n",
    "**Note** the Submit object was introduced in 8.5.6, which might be newer than your home cluster. The original API, using the `Schedd.submit` method, utilizes raw ClassAds and is not covered here.\n",
    "\n",
    "`Submit` objects consist of key-value pairs. Unlike ClassAds, the values do not have an inherent type (such as strings, integers, or booleans); they are evaluated with macro expansion at submit time. Where reasonable, they behave like Python dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== START ===\n",
      "foo = 1\n",
      "bar = 2\n",
      "baz = $(foo)\n",
      "qux = 3\n",
      "\n",
      "=== END ===\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sub = htcondor.Submit({\"foo\": \"1\", \"bar\": \"2\", \"baz\": \"$(foo)\"})\n",
    "sub.setdefault(\"qux\", \"3\")\n",
    "print(\"=== START ===\\n{}\\n=== END ===\".format(sub))\n",
    "print(sub.expand(\"baz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The available attribuets - and their semantics - are relatively well documented in the `condor_submit` [online help](http://research.cs.wisc.edu/htcondor/manual/v8.5/condor_submit.html); we wonâ€™t repeat them here. A minimal, but realistic submit object may look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "sub = htcondor.Submit({\"executable\": \"/bin/sleep\", \"arguments\": \"5m\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "To go from a submit object to job in a schedd, one must do three things:\n",
    "\n",
    "1.  Create a new transaction in the schedd using `transaction()`.\n",
    "2.  Call the `queue()` method, passing the transaction object.\n",
    "3.  Commit the transaction.\n",
    "\n",
    "Since the transaction object is a Python context, (1) and (3) can be achieved using Python's with statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "schedd = htcondor.Schedd()         # Create a schedd object using default settings.\n",
    "with schedd.transaction() as txn:  # txn will now represent the transaction.\n",
    "   print(sub.queue(txn))            # Queues one job in the current transaction; returns job's cluster ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "If the code block inside the `with` statement completes successfully, the transaction is automatically committed. If an exception is thrown (or Python abruptly exits), the transaction is aborted.\n",
    "\n",
    "By default, each invocation of `queue` will submit a single job.  A more common use case is to submit many jobs at once - often identical.  Suppose we don't want to submit a single \"sleep\" job, but 10; instead of writing a `for`-loop around the `queue` method, we can use the `count` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "schedd = htcondor.Schedd()                # Create a fresh Schedd object, pointint at the current schedd.\n",
    "with schedd.transaction() as txn:         # Start a new transaction\n",
    "    cluster_id = sub.queue(txn, count=10) # Submit 10 identical jobs\n",
    "print(cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query for all the jobs we have in the queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ ClusterId = 3; ProcId = 0; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ ClusterId = 3; ProcId = 1; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ ClusterId = 3; ProcId = 2; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ ClusterId = 3; ProcId = 3; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ ClusterId = 3; ProcId = 4; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ ClusterId = 3; ProcId = 5; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ ClusterId = 3; ProcId = 6; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ ClusterId = 3; ProcId = 7; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ ClusterId = 3; ProcId = 8; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ ClusterId = 3; ProcId = 9; EnteredCurrentStatus = 1562222630; JobStatus = 1; ServerTime = 1562222630 ]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedd.query(constraint='ClusterId=?={}'.format(cluster_id),\n",
    "             attr_list=[\"ClusterId\", \"ProcId\", \"JobStatus\", \"EnteredCurrentStatus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not entirely useful to submit many identical jobs -- but rather each one needs to vary slightly based on its ID (the \"process ID\") within the job cluster.  For this, the `Submit` object in Python behaves  similarly to submit files: references within the submit command are evaluated as macros at submit time.\n",
    "\n",
    "For example, suppose we want the argument to `sleep` to vary based on the process ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = htcondor.Submit({\"executable\": \"/bin/sleep\", \"arguments\": \"$(Process)m\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `$(Process)` string will be substituted with the process ID at submit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[ Args = \"0m\"; ClusterId = 4; ProcId = 0; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ Args = \"1m\"; ClusterId = 4; ProcId = 1; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ Args = \"2m\"; ClusterId = 4; ProcId = 2; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ Args = \"3m\"; ClusterId = 4; ProcId = 3; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ Args = \"4m\"; ClusterId = 4; ProcId = 4; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ Args = \"5m\"; ClusterId = 4; ProcId = 5; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ Args = \"6m\"; ClusterId = 4; ProcId = 6; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ Args = \"7m\"; ClusterId = 4; ProcId = 7; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ Args = \"8m\"; ClusterId = 4; ProcId = 8; JobStatus = 1; ServerTime = 1562222630 ],\n",
       " [ Args = \"9m\"; ClusterId = 4; ProcId = 9; JobStatus = 1; ServerTime = 1562222630 ]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with schedd.transaction() as txn:         # Start a new transaction\n",
    "    cluster_id = sub.queue(txn, count=10) # Submit 10 identical jobs\n",
    "print(cluster_id)\n",
    "schedd.query(constraint='ClusterId=?={}'.format(cluster_id),\n",
    "             attr_list=[\"ClusterId\", \"ProcId\", \"JobStatus\", \"Args\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The macro evaluation behavior (and the various usable tricks and techniques) are identical between the python bindings and the `condor_submit` executable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Jobs with Unique Inputs\n",
    "While it's useful to submit jobs which each differ by an integer, it is sometimes difficult to make your jobs fit into this paradigm.  A common case is to process unique files in a directory.  Let's start by creating a directory with 10 input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10 input files, each with unique content.\n",
    "import pathlib\n",
    "input_dir = pathlib.Path(\"input_directory\")\n",
    "input_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for idx in range(10):\n",
    "    input_file = input_dir / \"job_{}.txt\".format(idx)\n",
    "    input_file.write_text(\"Hello from job {}\".format(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create a python dictionary of all the filenames in the `input_directory` and pass the iterator to the `queue_with_itemdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'job_0.txt'}\n",
      "{'filename': 'job_1.txt'}\n",
      "{'filename': 'job_2.txt'}\n",
      "{'filename': 'job_3.txt'}\n",
      "{'filename': 'job_4.txt'}\n",
      "{'filename': 'job_5.txt'}\n",
      "{'filename': 'job_6.txt'}\n",
      "{'filename': 'job_7.txt'}\n",
      "{'filename': 'job_8.txt'}\n",
      "{'filename': 'job_9.txt'}\n"
     ]
    }
   ],
   "source": [
    "sub = htcondor.Submit({\"executable\": \"/bin/cat\"})\n",
    "sub[\"arguments\"] = \"$(filename)\"\n",
    "sub[\"transfer_input_files\"] = \"input_directory/$(filename)\"\n",
    "sub[\"output\"] = \"results.$(Process)\"\n",
    "\n",
    "# filter to select only the the job files\n",
    "itemdata = [{\"filename\": path.name} for path in input_dir.iterdir() if 'job' in path.name]\n",
    "\n",
    "for item in itemdata:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "with schedd.transaction() as txn:\n",
    "    # Submit one job per entry in the iterator.\n",
    "    results = sub.queue_with_itemdata(txn, 1, iter(itemdata))\n",
    "    \n",
    "print(results.cluster())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Warning*:\n",
    "As of the time of writing (HTCondor 8.9.2), this function takes an _iterator_ and not an _iterable_.  Therefore, `[1,2,3,4]` is not a valid third argument but `iter([1,2,3,4])` is; this restriction is expected to be relaxed in the future.\n",
    "\n",
    "Note that the results of the method is a `SubmitResults` object and not a plain integer as before.\n",
    "\n",
    "Next, we can make sure our arguments were applied correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ Out = \"results.0\"; JobStatus = 1; TransferInput = \"input_directory/job_0.txt\"; ServerTime = 1562222630; Args = \"job_0.txt\"; ClusterId = 5; ProcId = 0 ],\n",
       " [ Out = \"results.1\"; JobStatus = 1; TransferInput = \"input_directory/job_1.txt\"; ServerTime = 1562222630; Args = \"job_1.txt\"; ClusterId = 5; ProcId = 1 ],\n",
       " [ Out = \"results.2\"; JobStatus = 1; TransferInput = \"input_directory/job_2.txt\"; ServerTime = 1562222630; Args = \"job_2.txt\"; ClusterId = 5; ProcId = 2 ],\n",
       " [ Out = \"results.3\"; JobStatus = 1; TransferInput = \"input_directory/job_3.txt\"; ServerTime = 1562222630; Args = \"job_3.txt\"; ClusterId = 5; ProcId = 3 ],\n",
       " [ Out = \"results.4\"; JobStatus = 1; TransferInput = \"input_directory/job_4.txt\"; ServerTime = 1562222630; Args = \"job_4.txt\"; ClusterId = 5; ProcId = 4 ],\n",
       " [ Out = \"results.5\"; JobStatus = 1; TransferInput = \"input_directory/job_5.txt\"; ServerTime = 1562222630; Args = \"job_5.txt\"; ClusterId = 5; ProcId = 5 ],\n",
       " [ Out = \"results.6\"; JobStatus = 1; TransferInput = \"input_directory/job_6.txt\"; ServerTime = 1562222630; Args = \"job_6.txt\"; ClusterId = 5; ProcId = 6 ],\n",
       " [ Out = \"results.7\"; JobStatus = 1; TransferInput = \"input_directory/job_7.txt\"; ServerTime = 1562222630; Args = \"job_7.txt\"; ClusterId = 5; ProcId = 7 ],\n",
       " [ Out = \"results.8\"; JobStatus = 1; TransferInput = \"input_directory/job_8.txt\"; ServerTime = 1562222630; Args = \"job_8.txt\"; ClusterId = 5; ProcId = 8 ],\n",
       " [ Out = \"results.9\"; JobStatus = 1; TransferInput = \"input_directory/job_9.txt\"; ServerTime = 1562222630; Args = \"job_9.txt\"; ClusterId = 5; ProcId = 9 ]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedd.query(constraint='ClusterId=?={}'.format(results.cluster()),\n",
    "             attr_list=[\"ClusterId\", \"ProcId\", \"JobStatus\", \"TransferInput\", \"Out\", \"Args\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Managing Jobs\n",
    "\n",
    "Once a job is in queue, the schedd will try its best to execute it to completion. There are several cases where a user may want to interrupt the normal flow of jobs. Perhaps the results are no longer needed; perhaps the job needs to be edited to correct a submission error. These actions fall under the purview of _job management_.\n",
    "\n",
    "There are two `Schedd` methods dedicated to job management:\n",
    "\n",
    "* `edit()`: Change an attribute for a set of jobs to a given expression. If invoked within a transaction, multiple calls to `edit` are visible atomically.\n",
    "   * The set of jobs to change can be given as a ClassAd expression. If no jobs match the filter, _then an exception is thrown_.\n",
    "* `act()`: Change the state of a job to a given state (remove, hold, suspend, etc).\n",
    "\n",
    "Both methods take a _job specification_: either a ClassAd expression (such as `Owner=?=\"janedoe\"`)\n",
    "or a list of job IDs (such as `[\"1.1\", \"2.2\", \"2.3\"]`).  The `act` method takes an argument\n",
    "from the `JobAction` enum.  Commonly-used values include:\n",
    "\n",
    "* `Hold`: put a job on hold, vacating a running job if necessary.  A job will stay in the hold\n",
    "   state until explicitly acted upon by the admin or owner.\n",
    "* `Release`: Release a job from the hold state, returning it to Idle.\n",
    "* `Remove`: Remove a job from the Schedd's queue, cleaning it up first on the remote host (if running).\n",
    "   This requires the remote host to acknowledge it has successfully vacated the job, meaning ``Remove`` may\n",
    "   not be instantaneous.\n",
    "* `Vacate`: Cause a running job to be killed on the remote resource and return to idle state.  With\n",
    "  `Vacate`, jobs may be given significant time to cleanly shut down.\n",
    "\n",
    "Here's an example of job management in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== START JOB STATUS ===\n",
      "0: foo=bar, job_status = 1\n",
      "1: foo=bar, job_status = 1\n",
      "2: foo=default_string, job_status = 1\n",
      "3: foo=default_string, job_status = 1\n",
      "4: foo=default_string, job_status = 1\n",
      "=== END ===\n",
      "=== START JOB STATUS ===\n",
      "0: foo=bar, job_status = 1\n",
      "1: foo=bar, job_status = 1\n",
      "2: foo=default_string, job_status = 5\n",
      "3: foo=default_string, job_status = 5\n",
      "4: foo=default_string, job_status = 5\n",
      "=== END ===\n"
     ]
    }
   ],
   "source": [
    "with schedd.transaction() as txn:\n",
    "    clusterId = sub.queue(txn, 5)  # Queues 5 copies of this job.\n",
    "    schedd.edit([\"%d.0\" % clusterId, \"%d.1\" % clusterId], \"foo\", '\"bar\"') # Sets attribute foo to the string \"bar\".\n",
    "print(\"=== START JOB STATUS ===\")\n",
    "for job in schedd.xquery(requirements=\"ClusterId == %d\" % clusterId, projection=[\"ProcId\", \"foo\", \"JobStatus\"]):\n",
    "    print(\"%d: foo=%s, job_status = %d\" % (job.get(\"ProcId\"), job.get(\"foo\", \"default_string\"), job[\"JobStatus\"]))\n",
    "print(\"=== END ===\")\n",
    "\n",
    "schedd.act(htcondor.JobAction.Hold, 'ClusterId==%d && ProcId >= 2' % clusterId)\n",
    "print(\"=== START JOB STATUS ===\")\n",
    "for job in schedd.xquery(requirements=\"ClusterId == %d\" % clusterId, projection=[\"ProcId\", \"foo\", \"JobStatus\"]):\n",
    "    print(\"%d: foo=%s, job_status = %d\" % (job.get(\"ProcId\"), job.get(\"foo\", \"default_string\"), job[\"JobStatus\"]))\n",
    "print(\"=== END ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## That's It!\n",
    "\n",
    "You've made it through the very basics of the Python bindings.  While there are many other features the Python\n",
    "module has to offer, we have covered enough to replace the command line tools of `condor_q`, `condor_submit`,\n",
    "`condor_status`, `condor_rm` and others.\n",
    "\n",
    "Head back to the top-level notebook and try out one of our advanced tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
