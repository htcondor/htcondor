{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Advanced Schedd Interaction\n",
    "\n",
    "Launch this tutorial in a Jupyter Notebook on Binder: \n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/htcondor/htcondor-python-bindings-tutorials/master?urlpath=lab/tree/advanced/Advanced-Schedd-Interactions.ipynb)\n",
    "\n",
    "The introductory tutorial only scratches the surface of what the Python bindings\n",
    "can do with the ``condor_schedd``; this module focuses on covering a wider range\n",
    "of functionality:\n",
    "\n",
    "*  Job and history querying.\n",
    "*  Advanced job submission.\n",
    "*  Python-based negotiation with the Schedd.\n",
    "\n",
    "As usual, we start by importing the relevant modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import htcondor\n",
    "import classad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Job and History Querying\n",
    "\n",
    "In [HTCondor Introduction](../introductory/HTCondor-Introduction.ipynb), we covered the `Schedd.xquery` method\n",
    "and its two most important keywords:\n",
    "\n",
    "*  ``requirements``: Filters the jobs the schedd should return.\n",
    "*  ``projection``: Filters the attributes returned for each job.\n",
    "\n",
    "For those familiar with SQL queries, ``requirements`` performs the equivalent\n",
    "as the ``WHERE`` clause while ``projection`` performs the equivalent of the column\n",
    "listing in ``SELECT``.\n",
    "\n",
    "There are two other keywords worth mentioning:\n",
    "\n",
    "*  ``limit``: Limits the number of returned ads; equivalent to SQL's ``LIMIT``.\n",
    "*  ``opts``: Additional flags to send to the schedd to alter query behavior.\n",
    "   The only flag currently defined is `QueryOpts.AutoCluster`; this\n",
    "   groups the returned results by the current set of \"auto-cluster\" attributes\n",
    "   used by the pool.  It's analogous to ``GROUP BY`` in SQL, except the columns\n",
    "   used for grouping are controlled by the schedd.\n",
    "\n",
    "To illustrate these additional keywords, let's first submit a few jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "schedd = htcondor.Schedd()\n",
    "sub = htcondor.Submit({\n",
    "                       \"executable\": \"/bin/sleep\",\n",
    "                       \"arguments\":  \"5m\",\n",
    "                       \"hold\":       \"True\",\n",
    "                      })\n",
    "with schedd.transaction() as txn:\n",
    "    clusterId = sub.queue(txn, 10)\n",
    "print(clusterId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "**Note:** In this example, we used the ``hold`` submit command to indicate that\n",
    "the jobs should start out in the ``condor_schedd`` in the *Hold* state; this\n",
    "is used simply to prevent the jobs from running to completion while you are\n",
    "running the tutorial.\n",
    "\n",
    "We now have 10 jobs running under ``clusterId``; they should all be identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(sum(1 for _ in schedd.xquery(projection=[\"ProcID\"], requirements=\"ClusterId==%d\" % clusterId, limit=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The ``sum(1 for _ in ...)`` syntax is a simple way to count the number of items\n",
    "produced by an iterator without buffering all the objects in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Querying many Schedds\n",
    "\n",
    "On larger pools, it's common to write Python scripts that interact with not one but many schedds.  For example,\n",
    "if you want to implement a \"global query\" (equivalent to ``condor_q -g``; concatenates all jobs in all schedds),\n",
    "it might be tempting to write code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "for schedd_ad in htcondor.Collector().locateAll(htcondor.DaemonTypes.Schedd):\n",
    "    schedd = htcondor.Schedd(schedd_ad)\n",
    "    jobs += schedd.xquery()\n",
    "print(len(jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "This is sub-optimal for two reasons:\n",
    "\n",
    "*  ``xquery`` is not given any projection, meaning it will pull all attributes for all jobs -\n",
    "   much more data than is needed for simply counting jobs.\n",
    "*  The querying across all schedds is serialized: we may wait for painfully long on one or two\n",
    "   \"bad apples.\"\n",
    "\n",
    "We can instead begin the query for all schedds simultaneously, then read the responses as\n",
    "they are sent back.  First, we start all the queries without reading responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "queries = []\n",
    "coll_query = htcondor.Collector().locateAll(htcondor.DaemonTypes.Schedd)\n",
    "for schedd_ad in coll_query:\n",
    "    schedd_obj = htcondor.Schedd(schedd_ad)\n",
    "    queries.append(schedd_obj.xquery())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The iterators will yield the matching jobs; to return the autoclusters instead of jobs, use\n",
    "the ``AutoCluster`` option (``schedd_obj.xquery(opts=htcondor.QueryOpts.AutoCluster)``).  One\n",
    "auto-cluster ad is returned for each set of jobs that have identical values for all significant\n",
    "attributes.  A sample auto-cluster looks like:\n",
    "\n",
    "       [\n",
    "        RequestDisk = DiskUsage;\n",
    "        Rank = 0.0;\n",
    "        FileSystemDomain = \"hcc-briantest7.unl.edu\";\n",
    "        MemoryUsage = ( ( ResidentSetSize + 1023 ) / 1024 );\n",
    "        ImageSize = 1000;\n",
    "        JobUniverse = 5;\n",
    "        DiskUsage = 1000;\n",
    "        JobCount = 1;\n",
    "        Requirements = ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( ( TARGET.HasFileTransfer ) || ( TARGET.FileSystemDomain == MY.FileSystemDomain ) );\n",
    "        RequestMemory = ifthenelse(MemoryUsage isnt undefined,MemoryUsage,( ImageSize + 1023 ) / 1024);\n",
    "        ResidentSetSize = 0;\n",
    "        ServerTime = 1483758177;\n",
    "        AutoClusterId = 2\n",
    "       ]\n",
    "\n",
    "We use the `poll` function, which will return when a query has available results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10 results from jovyan@63a78c05ff13.\n",
      "{'jovyan@63a78c05ff13': 10}\n"
     ]
    }
   ],
   "source": [
    "job_counts = {}\n",
    "for query in htcondor.poll(queries):\n",
    "    schedd_name = query.tag()\n",
    "    job_counts.setdefault(schedd_name, 0)\n",
    "    count = len(query.nextAdsNonBlocking())\n",
    "    job_counts[schedd_name] += count\n",
    "    print(\"Got {} results from {}.\".format(count, schedd_name))\n",
    "print(job_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The `QueryIterator.tag` method is used to identify which query is returned; the\n",
    "tag defaults to the Schedd's name but can be manually set through the ``tag`` keyword argument\n",
    "to `Schedd.xquery`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## History Queries\n",
    "\n",
    "After a job has finished in the Schedd, it moves from the queue to the history file.  The\n",
    "history can be queried (locally or remotely) with the `Schedd.history` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    [\n",
      "        JobStatus = 3; \n",
      "        ProcId = 9; \n",
      "        ClusterId = 5\n",
      "    ]\n",
      "\n",
      "    [\n",
      "        JobStatus = 3; \n",
      "        ProcId = 4; \n",
      "        ClusterId = 6\n",
      "    ]\n"
     ]
    }
   ],
   "source": [
    "schedd = htcondor.Schedd()\n",
    "for ad in schedd.history('true', ['ProcId', 'ClusterId', 'JobStatus', 'WallDuration'], 2):\n",
    "    print(ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "At the time of writing, unlike `Schedd.xquery`, `Schedd.history`\n",
    "takes positional arguments and not keyword.  The first argument a job constraint; second is the\n",
    "projection list; the third is the maximum number of jobs to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Advanced Job Submission\n",
    "\n",
    "In [HTCondor Introduction](../introductory/HTCondor-Introduction.ipynb), we introduced the `Submit` object.  `Submit`\n",
    "allows jobs to be created using the *submit file* language.  This is the well-documented, familiar\n",
    "means for submitting jobs via ``condor_submit``.  This is the preferred mechansim for submitting\n",
    "jobs from Python.\n",
    "\n",
    "Internally, the submit files are converted to a job ClassAd.  The older `Schedd.submit`\n",
    "method allows jobs to be submitted as ClassAds.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "schedd = htcondor.Schedd()\n",
    "job_ad = classad.ClassAd({\n",
    "     'Cmd': '/bin/sh',\n",
    "     'JobUniverse': 5,\n",
    "     'Iwd': os.path.abspath(\"/tmp\"),\n",
    "     'Out': 'testclaim.out',\n",
    "     'Err': 'testclaim.err',\n",
    "     'Arguments': 'sleep 5m',\n",
    "})\n",
    "clusterId = schedd.submit(job_ad, count=2)\n",
    "print(clusterId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "This will submit two copies of the job described by ``job_ad`` into a single job cluster.\n",
    "\n",
    "**Hint**: To generate an example ClassAd, take a sample submit description\n",
    "file and invoke:\n",
    "\n",
    "      condor_submit -dump <filename> [cmdfile]\n",
    "\n",
    "Then, load the resulting contents of ``<filename>`` into Python.\n",
    "\n",
    "Calling `Schedd.submit` standalone will automatically create and commit a transaction.\n",
    "Multiple jobs can be submitted atomically and more efficiently within a `Schedd.transaction()`\n",
    "context.\n",
    "\n",
    "Each `Schedd.submit` invocation will create a new job cluster; all attributes will be\n",
    "identical except for the ``ProcId`` attribute (process IDs are assigned in monotonically increasing order,\n",
    "starting at zero).  If jobs in the same cluster need to differ on additional attributes, one may use the\n",
    "`Schedd.submitMany` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "foo = classad.ClassAd({'myAttr': 'foo'})\n",
    "bar = classad.ClassAd({'myAttr': 'bar'})\n",
    "clusterId = schedd.submitMany(job_ad, [(foo, 2), (bar, 2)])\n",
    "print(clusterId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    [\n",
      "        ServerTime = 1589900105; \n",
      "        ProcId = 0; \n",
      "        myAttr = \"foo\"\n",
      "    ]\n",
      "\n",
      "    [\n",
      "        ServerTime = 1589900105; \n",
      "        ProcId = 1; \n",
      "        myAttr = \"foo\"\n",
      "    ]\n",
      "\n",
      "    [\n",
      "        ServerTime = 1589900105; \n",
      "        ProcId = 2; \n",
      "        myAttr = \"bar\"\n",
      "    ]\n",
      "\n",
      "    [\n",
      "        ServerTime = 1589900105; \n",
      "        ProcId = 3; \n",
      "        myAttr = \"bar\"\n",
      "    ]\n"
     ]
    }
   ],
   "source": [
    "query = schedd.xquery('ClusterId=={}'.format(clusterId), ['ProcId', 'myAttr'])\n",
    "for ad in query:\n",
    "    print(ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "`Schedd.submitMany` takes a basic job ad (sometimes referred to as the *cluster ad*),\n",
    "shared by all jobs in the cluster and a list of *process ads*.  The process ad list indicates\n",
    "the attributes that should be overridden for individual jobs, as well as the number of such jobs\n",
    "that should be submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Job Spooling\n",
    "\n",
    "HTCondor file transfer will move output and input files to and from the submit host; these files will\n",
    "move back to the original location on the host.  In some cases, this may be problematic; you may want\n",
    "to submit one set of jobs to run ``/home/jovyan/a.out``, recompile the binary, then submit a fresh\n",
    "set of jobs.  By using the *spooling* feature, the ``condor_schedd`` will make a private copy of\n",
    "``a.out`` after submit, allowing the user to make new edits.\n",
    "\n",
    "**Hint**: Although here we give an example of using `Schedd.spool` for spooling on\n",
    "   the local Schedd, with appropriate authoriation the same methods can be used for submitting to\n",
    "   remote hosts.\n",
    "\n",
    "To spool, one must specify this at submit time and invoke the `Schedd.spool` method\n",
    "and provide an ``ad_results`` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ ClusterId = 10; LocalSysCpu = 0.0; RemoteSysCpu = 0.0; OnExitRemove = true; PeriodicRemove = false; LocalUserCpu = 0.0; RemoteUserCpu = 0.0; StreamErr = false; HoldReason = \"Spooling input data files\"; CommittedSuspensionTime = 0; ProcId = 0; PeriodicHold = false; CondorPlatform = \"$CondorPlatform: X86_64-CentOS_5.11 $\"; ExitStatus = 0; ShouldTransferFiles = \"YES\"; LastSuspensionTime = 0; NumJobStarts = 0; NumCkpts = 0; WhenToTransferOutput = \"ON_EXIT\"; TargetType = \"Machine\"; JobNotification = 0; BufferSize = 524288; ImageSize = 100; PeriodicRelease = false; CompletionDate = 0; RemoteWallClockTime = 0.0; Arguments = \"sleep 5m\"; WantCheckpoint = false; NumSystemHolds = 0; CumulativeSuspensionTime = 0; QDate = 1589900105; EnteredCurrentStatus = 1589900105; CondorVersion = \"$CondorVersion: 8.9.6 Apr 07 2020 BuildID: UW_Python_Wheel_Build $\"; MyType = \"Job\"; Owner = undefined; ExitBySignal = false; JobUniverse = 5; BufferBlockSize = 32768; Err = \"testclaim.err\"; NiceUser = false; CoreSize = -1; CumulativeSlotTime = 0; OnExitHold = false; WantRemoteSyscalls = false; CommittedTime = 0; Cmd = \"/bin/sh\"; WantRemoteIO = true; StreamOut = false; CommittedSlotTime = 0; TotalSuspensions = 0; JobPrio = 0; CurrentHosts = 0; RootDir = \"/\"; Out = \"testclaim.out\"; LeaveJobInQueue = JobStatus == 4 && (CompletionDate is UNDDEFINED || CompletionDate == 0 || ((time() - CompletionDate) < 864000)); RequestCpus = 1; RequestDisk = DiskUsage; MinHosts = 1; Requirements = true && TARGET.OPSYS == \"LINUX\" && TARGET.ARCH == \"X86_64\" && TARGET.HasFileTransfer && TARGET.Disk >= RequestDisk && TARGET.Memory >= RequestMemory && TARGET.Cpus >= RequestCpus; RequestMemory = ifthenelse(MemoryUsage isnt undefined,MemoryUsage,(ImageSize + 1023) / 1024); Args = \"\"; MaxHosts = 1; JobStatus = 5; DiskUsage = 1; In = \"/dev/null\"; HoldReasonCode = 16; Iwd = \"/tmp\"; NumJobCompletions = 0; NumRestarts = 0 ]]\n"
     ]
    }
   ],
   "source": [
    "ads = []\n",
    "cluster = schedd.submit(job_ad, 1, spool=True, ad_results=ads)\n",
    "schedd.spool(ads)\n",
    "print(ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "This will copy the files into the Schedd's ``spool`` directory.  After the job completes, its\n",
    "output files will stay in the spool.  One needs to call `Schedd.retrieve` to\n",
    "move the outputs back to their final destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "schedd.retrieve(\"ClusterId=={}\".format(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's clean up after ourselves (this will remove all of the jobs you own from the queue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ TotalJobAds = 15; TotalPermissionDenied = 0; TotalAlreadyDone = 0; TotalNotFound = 0; TotalSuccess = 13; TotalChangedAds = 1; TotalBadStatus = 0; TotalError = 0 ]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "schedd.act(htcondor.JobAction.Remove, f'Owner==\"{getpass.getuser()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
